---
title: Testing for Structural Breaks in Factor Copula Models - Implementation and Application in Social Media Topic Analysis
author: Malte Bonart
type: Master thesis
logo: "UoC-Logo.eps"
institue: "Submitted for the Master Examination in Economics at the Faculty of Management, Economics and Social Sciences of the University of Cologne in June 2018."
supervisor: "Prof.Dr. Dominik Wied"
bibliography: "./literature.bib"
output: 
  pdf_document:
    highlight: tango
    #latex_engine: xelatex
    latex_engine: pdflatex
    template: thesis_template.tex
    citation_package: biblatex
    number_sections: true
abstract: "This is my abstract."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "\\textwidth")
#install.packages("devtools")
#devtools::install_github("bonartm/factorcopula")
```

# Introduction
\label{s:introduction}

Many multivariate statistical models are based on the assumption that the variables follow a multivariate Gaussian distribution. Here, the natural measure of dependece is the covariance or correlation matrix between the variables \parencite[p. 25]{Joe2015}. But using simple correlations as the only dependence measure can be misleading. In many cases real world data does not follow a Gaussian distribution and shows other distributional feautures, such as skeweness or heavy tails. Correlation requires that the variance of the marginal distribution is finite. This requirement can be problematic when dealing with heavy tailed distributions. Further more, correlation is only a measure of linear dependence and it is not invariant under monotonic transformations of the variables \parencite[6-8]{Embrechts2002}. This means, that by applying a strictly increasing function (for example the $\log$ function) to each variable, the correlation matrix changes.

Due to this shortcomings of the ordinary linear correlation coefficient, other and more flexible methods for describing the dependence structure between non Gaussian variables have been developed. In this area of research, models based on so called copula functions became increasingly popular since the 1990th \parencite[p. 1]{Nelsen1999}. Up to the year 1999, only 38 publications which include the topics \emph{copula} and \emph{dependence} have been published in peer-reviewed journals. From 2000 - 2008 the number of copula related publications was 334 and for the years 2009 - 2017 2048 publications are listed.\footnote{The numbers are taken from the \emph{Web of Science Core Collection} for the search term \texttt{TOPIC: (copula) AND TOPIC: (dependence)}.} 

At first, any multivariate distribution function for which its marginal distributions have a uniform distribution on the interval $[0, 1]$ can be called a copula function \parencite[p. 1]{Nelsen1999}. But its popularity is based on the results of a thereom by \textcite{Sklar1959}: It implies that multivariate distributions can be constructed by separately specifying the marginal distributions of the random variables and by defining the dependence structure among the variables. The dependence structure is described with a copula function. Thus, a copula links the multivariate distribution function to its marginal distributions. In case of continous multivariate distributions the copula function is uniquely defined.

Due to this results, copulas are mainly used in two ways: First, to model the dependence structure of multivariate distributions independent of their underlying marginal distributions and second, to construct bivariate or multivariate distributions based on a copula function and marginal distributions \parencite[302-304]{Sempi2011}. 

This allows for a two-stage estimation process in multivariate models in which the marginal distributions and the copula function is estimated separately. By doing so, semiparametric techniques can be utilized. For example, the marginal distribution can be estimated using the empirical distribution function while the copula function is estimated parametrically. \parencite[777]{Patton2009}.

For time series data copula theory can be used in two ways: First, to describe the cross sectional dependence structure by estimating the conditional copula function of the conditional joint distribution at some timepoint given past information. Second, copulas can be used to describe the dependence between observations of a univariate time series. This is related to the study of Markov processes. \parencite[p. 771 ff]{Patton2009}.

This thesis summarizes and strucutures the current developement in the field of factor copula models. It contributes to the sicentific disucssion in two ways: First, a software library for a consistent specification, simulation and estimation of factor copula models is presented. With the library, the methods and the strucutral break test are made avalaible to a broader scientific audience and can be used by applied reasearchers. Second, we study novel ways of applying the discussed methods to areas outside of the finance community. The strucutral break test is applied to an aggregated dataset of social media posts from german politicians and political parties.






# Theoretical foundation

In this chapter we present and summarize the theoretical foundation of this thesis. First, the general idea behind copula functions is introduced. Second, a special class of copula models, the so called \emph{factor copula} model is presented. Third, we discuss copula models in the context of time series data and present a specific framework for modelling dependencies of multivariate time series. Fourth, the \emph{simulated methods of moments} is explained. It is the estimation method which is used throughout this work. Finally we summarize the ideas of a structural break test for possibly time varying parameters of a factor copula model. 

## Copula theory
\label{s:copulatheory}


The joint cumulative distribution function (cdf) $F_Y(y_1, \dots, y_N) = P(Y_1 \leq y_1, \dots, Y_N \leq y_N)$ for some multivariate random vector $Y$ of dimension $N$ has the continous marginal distributions $F_{Y_i}(y_i) = P(Y_i \leq y_i) ~\forall~ i = 1, \dots, N$. Estimating $F_Y$ is computationally demanding espacialy if $N$ becomes large. Therefore, a copula function is introduced which can be used to link the marginal and the joint cdf.

A function of the type $C: [0, 1]^N \rightarrow [0,1]$, with $N \geq 2$ is called a \emph{copula} if it is the distribution function of a random vector $U$ such that $C_U(u_1, \dots, u_N) = P(U_1 \leq u_1, \dots, U_N \leq u_N)$ and if its marginal distributions are $U_i \sim Unif(0, 1)$, e.g. uniformly distributed \parencite[p. 7]{Joe2015}. The thereom by \textcite{Sklar1959} states, that every d-variate distribution function $F_Y(y_1,\dots, y_n)$ can be eypressed in terms of its marginal distributions and a copula function such that 
\begin{equation}
\label{eq:sklar}
F_Y(y_1, \dots, y_N) = C_U(F_1(y_1), \dots, F_N(y_N)).
\end{equation}
To see this, consider the so called probability-integral transformation which states that the marginal random variables $U_i = F_{Y_i}(Y_i)$ are uniformly distributed \parencite[4]{Embrechts2002}. This is due to the fact that $F_{U_i}(u_i) = P(U_i \leq u) = P(F_{Y_i}(Y_i) \leq u_i) = P(Y_i \leq F_{Y_i}^{-1}(u_i)) = F_{Y_i}(F_{Y_i}^{-1}(u_i)) = u_i$ is the distribution function of a $Unif(0,1)$ distributed variable. Using this transformation we can write
\begin{align}
\label{eq:proofsklar}
\begin{split}
F_Y(y_1, \dots, y_N) &= P(Y_1 \leq y_1, \dots, Y_N \leq y_N)\\
&= P(F_{Y_1}(Y_1) \leq F_{Y_1}(y_1), \dots, F_{Y_N}(Y_N) \leq F_{Y_N}(y_N)) \\
&= P(U_1 \leq F_{Y_1}(y_1), \dots, U_N \leq F_{Y_N}(y_N)) \\
&= C_U(F_{Y_1}(y_1), \dots, F_{Y_N}(y_N)).
\end{split}
\end{align}

Note that transforming the marginal distributions to a $Unif(0, 1)$ distribution is somehow an arbitrary choice and other possibilities, for instance the Gaussian distribution, are also possible.  \parencite[7]{Mikosch2006}

If $F_Y$ is \emph{continuous} with marginal quantile functions $F^{-1}_{Y_1}, \dots, F^{-1}_{Y_N}$ then the copula function $C_U(\mathbf{u})$ is uniquely determined by $C(\mathbf{u}) = F(F_{Y_1}^{-1}(u_1), \dots, F_{Y_N}^{-1}(u_N))$. If not all marginal distribution are continous the copula still exist but in this case it is not unique anymore \parencite[4-5]{Embrechts2002}.

The simplest form of a copula function is the copula of a vector of independent variables for wich we can write $F_Y(\bm{y}) =  \prod_{i = 1}^N F_{Y_i}(y_i)$. Using the derivation in \eqref{eq:proofsklar}, this results in the independence copula 
\begin{equation}
C_{ind}(u_1, \dots, u_N) = \prod_{i = 1}^N u_i.
\end{equation}

Many other possible funtional forms for a copula $C_U$ exist. In this paper, we focus on a special class, the so called factor copulas which we present in section \ref{s:factorcopula}. 

Copulas are usefull for the study of dependence between a set of random variables, because the copula function which defines the dependency structure is unaffected by monotonic transformations of the marginal variables. In contrast to the linear correlation coefficient, this property allows the definition of alternative scale-invarant dependence measures \parencite[125]{Nelsen1999}. 

To see the scale-invaraint property of the copula function, consider a random vector $U$ with copula distribution function $C_U(\bm{u})$ and $C_U(F_1(y_1), \dots, F_N(y_N)) = F_Y(y_1, \dots, y_N)$. By applying some increasing function $T_i(Y_i)$ to the random variables and using \eqref{eq:proofsklar} we can write
\begin{equation}
\begin{split}
F_{T(Y)}(\bm{y}) &= P(T_1(Y_1) \leq y_1, \dots, T_N(Y_N) \leq y_N)\\
&=  P(T_1(F_{y_1}^{-1}(U_1)) \leq y_1, \dots, T_N(F_{y_N}^{-1}(U_N)))\\
&= C_U(F_{T_1(Y_1)}(y_1), \dots, F_{T_N(Y_N)}(y_N)).
\end{split}
\end{equation}
Thus, allthough $Y$ and $T(Y)$ have different joint distribution functions, they share the same copula function. 

In the following we shortly present three common scale invariant measures of dependency between two variables $Y_i$ and $Y_j$. In contrast to the linear correlation coefficient, these measures can be expressed solely as a function of the underlying bivariate copula.\footnote{For a proof of the copula representations see \textcite[16-18]{Embrechts2002}.} Spearman's and Kendall's rank correlation both measure the degree of monotonic dependence between the two variables $Y_i$ and $Y_j$ with joint distribution function $F_Y(\bm{y})$. As the linear correlation coefficient they are symmetric and normalised in the interval $[0, 1]$ \parencite[15]{Embrechts2002}.

Kendall's rank correlation is based on the definition of concoradance and discordance: Consider two pairs of independent and identically (iid) distributed random vectors $(Y_i^1, Y_j^1)$ and $(Y_i^2, Y_j^2)$. Both pairs share the same joint distribution $F_Y$. The pair is concordant if $(Y_i^1 - Y_i^2)(Y_j^1-Y_j^2) > 0$ and discordant if $(Y_i^1-Y_i^2)(Y_j^1-Y_j^2) < 0$. In words, for the former case large (small) values of one pair occur with large (small) values of the other. For the latter case, large (small) values of one pair occur with small (large) values of the other \parencite[125-126]{Nelsen1999}.

With this definition Kendalls's rank correlation is given as the probability of concordance minus the probability of discordance:
\begin{align}
\begin{split}
\label{eq:kendalls}
\tau_{i,j} &= P((Y_i^1-Y_i^2)(Y_j^1-Y_j^2) < 0) - P((Y_i^1-Y_i^2)(Y_j^1-Y_j^2) > 0)\\
&= 4\int\int_{[0,1]^2} C(u_i, u_j)dC(u_i, u_j)-1.
\end{split}
\end{align}

Spearman's rank correlation is given as the ordinary correlation coefficient between the probability-integral transforms $F_{Y_i}(Y_i)$ and $F_{Y_j}(Y_j)$:
\begin{equation}
\label{eq:spearman}
\rho_{i,j}^S = \rho(F_{Y_i}(Y_i), F_{Y_j}(Y_j)) = 12\int\int_{[0,1]^2} u_iu_j dC(u_i, u_j)-3.
\end{equation}

Finally, to capture dependencies in the joint lower or joint upper parts of the distribution, one defines the coefficients of upper and lower tail dependency: 
\begin{align}
\label{eq:taildependency}
\begin{split}
\tau^U_{i, j} &= \underset{q \rightarrow 1}{\lim} P(Y_i > F_{Y_i}^{-1}(q) | Y_j > F_{Y_j}^{-1}(q)) 
= \underset{q \rightarrow 1}{\lim} \frac{1-2q+C(q,q)}{1-q},\\
\tau^L_{i, j} &= \underset{q \rightarrow 0}{\lim} P(Y_i \leq F_{Y_i}^{-1}(q) | Y_j \leq F_{Y_j}^{-1}(q))
= \underset{q \rightarrow 0}{\lim} \frac{C(q,q)}{q}.
\end{split}
\end{align}
The coefficients measures the probability that extreme large (small) values occur in one variable, given extreme large (small) values in the other variable. In contrast to other common dependence measures, the coefficients of upper and lower tail dependency are defined on the interval $[0, 1]$. Since the limit only exists theoretically and not for observable data, one usually calculates upper and lower \emph{quantile} dependency for some values of $q$ close to $0$ and $1$ \parencite[62-63]{Joe2015}.


## Factor copulas
\label{s:factorcopula}

Factor copulas are a special class of copula models for which the copula function $C_U(u_1, \dots, u_N)$ is based on a latent factor structure as defined in \textcite{Patton2013, Patton2017}. 

Consider a set of artificial variables $X_i, i = 1, \dots,N$ which linearly depend on some latent factors $Z_k, k = 1, \dots, K$ and some iid distributed error $e_i$ such that $X_i =  \sum_{k = 1}^{K}\beta_{ik}Z_k+ \epsilon_i$. The linear coefficients $\beta_{ik}$ are also called factor loadings. The latent variables $Z_k$ and the error term $\epsilon_i$ follow some parametrized distributions with parameter vectors $\bm{\gamma}_\epsilon$ and $\bm{\gamma}_{Z_k}$ and we write: $\epsilon_i \overset{iid}{\sim} F_\epsilon(\bm{\gamma_\epsilon})$ and $Z_k \sim F_{Z_k}(\bm{\gamma_{Z_k}})$. While the variables $X_i$ usually dependent on each other, the latent factors are independent from each other and from the error term. 

As shown in the previous section, the joint probability function $F_{X}(x_1, \dots, x_N)$ of the artificial variables can be expressed in terms of its marginal distributions $F_{X_i}(x)$ and a factor copula function $C_U(u_1, \dots, u_N)$ such that $F_{X}(x_1, \dots, x_N) = C_U(F_{X_1}(x_1), \dots, F_{X_N}(x_N); \bm{\theta})$.

The artifical variables $X_i$ are only used to construct the factor copula function $C_U(u_1, \dots, u_N)$. The parameters of the factor structure are chosen in such a way that the resulting copula function fits the copula of the observable data $\bm{Y}$, such that $F_{Y}(y_1, \dots, y_n) = C_U(F_{Y_1}(y_1),\dots, F_{Y_N}(y_N))$. Once the factor copula function is approximated, the artificial variables and its marginal distributions $F_{X_i}(x)$ are of no interest.

The parameters of the factor model are collected in a parameter vector $\bm{\theta} = (\beta_{11}, \dots, \beta_{i1}, \dots \beta_{ik}, \bm{\gamma_{Z_1}}', \dots, \bm{\gamma}_{Z_K}', \bm{\gamma}_\epsilon')'$. It consists of all linear coefficients and the distributional parameters of the error term and the latent variables. The number of latent variables $K$ and the distribution functions $F_{Z_1}, \dots, F_{Z_k}, F_\epsilon$ are hyper-parameters of the model which have to be chosen prior to the estimation.\footnote{\textcite[p. 143ff]{Patton2017} provide a heuristic of finding the number of latent variables by analyzing so called \emph{scree-plots}: Ordered eigenvalues from the sample rank-correlation matrix of the data.}

Using matrix notation, the model can be summarized in the following set of equations:
\begin{align}
\label{eq:factorcopula}
\begin{split}
\bm{Y} &= (Y_1, \dots, Y_N)' \\
\bm{X} &= (X_1, \dots, X_N)' = \bm{\beta Z} + \bm{\epsilon}\\
F_{Y}(\bm{y}) &= C_U(F_{Y_1}(y_1),\dots, F_{Y_N}(y_N); \bm{\theta})\\
F_{X}(\bm{x}) &= C_U(F_{X_1}(x_1), \dots, F_{X_N}(x_N); \bm{\theta})
\end{split}
\end{align}

To model the joint probability $F_Y(\bm{y})$, a two-stage estimation process can be used: First, the marginal distributions $\hat{F}_{Y_i}$ are estimated parametrically or non-parametrically, e.g. by using some parametric model or the empirical distribution function. Second, the factor structure for the copula function is fitted to the data by finding the optimal $\hat{\bm{\theta}}$. Usually, a closed form of the factor copula does not exist. Therefore, one has to rely on simulation based estimation methods as described in section \ref{s:SMM}. 

This aproach allows for a variety of different dependence structures and can be applied to high dimensional data. An upper bound for the number of model parameters $P = |\bm{\theta}|$ to be estimated is given by the size of the factor matrix and the number of additional free distributional paramters such that $P \leq (N \dot K + |\bm{\gamma_{Z_1}}| + \dots + |\bm{\gamma_{Z_K}}| + |\bm{\gamma_\epsilon}|)$. To reduce the number of parameters, \textcite[148, 150]{Patton2017} present two restrictions on the matrix of factor loadings $\bm{\beta}$: the restrictive \emph{equidependence} and the less restrictive \emph{block-equidependence} model. 

For the first model, it is assumed that $K = 1$ and $\bm{\beta} = (\beta, \dots, \beta)'$. Thus, the model consists of a single latent factor and a single factor loading $\beta$ which is the same for all variables. This implies equal pairwise dependencies for all observable variables.  

\begin{figure}[H]
	\includegraphics[width=\textwidth]{./figures/fig1}
	\caption[Illustration of different factor copula models.]{Illustration of different equidependence factor copula models with $N = 2$, $\beta = 1.5$, $Y_i \sim N(0, 1)$ and different distributions for the latent variable and the error term.}	
	\label{f:equidependence}  
\end{figure}

Figure \ref{f:equidependence} shows four different simulations from a two dimensional one factor equidependence factor copula model. The marginal distributions are standard normal, the linear coefficient is fixed at $\beta = 1.5$ but the distributions of the latent variable and the error term differ. All models produce positive dependence but the symmetry and tail dependency differs through the choice of the distributions. The upper left panel are realizations from a factor copula model with a skew-t distributed latent factor and a t-distributed error term. The degrees of freedom are set to $df = 4$ and the skewness parameter to $\lambda = -0.8$. This produces strong assymetric tail dependencies. The bottom left panel produces symmetric tail dependencies since $\lambda = 0$. This results in an ordinary t-distribution for the latent variable. The bottom right panel shows realizations from a gaussian copula which results in a multivariate gaussian distribution with no tail dependency. The last panel shows the combination of an exponential latent variable with a normal distributed error term. 

The second restriction, the block-equidependence model, is less restrictive that the equidependence model and is suitable for variables which can be naturally partitioned into different groups.\footnote{E.g. this could be stock market prices grouped into different industry sectors.} The model assumes a common factor for all groups and a group specific factor for each group. Thus, each variable is only affected by two factors. For the matrix of factor loadings, it is further assumed that all variables in the same group have the same factor loading while variables in different groups can have different loadings. This implies equal pairwise intra-group dependencies while the pairwise inter-group dependencies can vary between the groups. 

Formally, consider a partition of $\bm{Y} = (Y_1, \dots, Y_N)'$ into $M$ groups. A single variable can then be written as $Y_i^j$, where $i = 1, \dots, N$, $j = 1, \dots, M$. The value $k_j$ is the number of variables in group $j$ and it holds $\sum_{j = 1}^M k_j = N$. Then the factor copula model can be summarized as:

\begin{align}
\label{eq:block-equidependence}
\begin{split}
\bm{X} &= (X^1_1, \dots, X_{k_1}^1, X_{k_1+1}^2, \dots, X_{k_1+k_2}^2, \dots, \dots, X_N^{M})' = \bm{\beta Z} + \bm{\epsilon}\\
\bm{Z} &= (Z_0, Z_1, \dots, Z_M)'\\
X_i^j &= \beta_j Z_0 + \beta_{M+j} Z_j + \epsilon_i\\
\bm{\beta} &= \begin{pmatrix}
\beta^1	& \beta^{M+1} & 0			&\cdots & 0 \\
\beta^1 & \beta^{M+1} & 0			&\cdots & 0 \\
\vdots 	& \vdots 	  & \vdots 		&\ddots & \vdots \\
\beta^1 & \beta^{M+1} & 0			&\cdots & 0 \\
\beta^2 & 0  		  & \beta^{M+2}	&\cdots & 0  \\
\vdots 	& \vdots 	  & \vdots		&\ddots & \vdots \\
\beta^M & 0			  & 0			&\cdots & \beta^{M+M}\\
\vdots 	& \vdots 	  & \vdots		&\ddots & \vdots \\
\beta^M & 0			  & 0			&\cdots & \beta^{M+M}\\
\end{pmatrix},
\end{split}
\end{align}
where the matrix $\bm{\beta}$ is of size $N \times (M+1)$ but with only $2M$ actual factor loadings. 

## Copula models for multivariate time series
\label{s:dynamiccopula}

Up to now, it was assumed, that the data generating process (DGP) for the copula model is static and time invariant. But one can also extent the copula model to univariate or multivariate time series processes. For the former case, the copula is used to estimate the joint distribution of a one dimensional time series $(Y_t, Y_{t+1}, \dots, Y_{t+n})'$. For the latter, the interest lies in the conditional joint distribution of the time dependent random vector $\bm{Y_t} = (Y_{1t}, \dots, Y_{Nt})'$. The conditional cdf can be written as $F_{\bm{Y}_t|\mathcal{F}_{t-1}}(\bm{y})$,  where the $\sigma$-algebra $\mathcal{F}_{t-1}$ possibly contains past information and information from other exogenous variables $\{\bm{Y}_{t-1}', \bm{Y}_{t-2}', \dots, \bm{X}_{t}', \bm{X}_{t-1}', \dots\}$ \parencite[4-6]{Patton2012}. 

Sklar's theorem given by \eqref{eq:sklar} shows how the copula function links together the marginal and the joint cdf. It can be extended to the multivariate time series case in which the conditional distribution is split into a conditional copula $C_{U_t|\mathcal{F}_{t-1}}(\bm{u})$ and conditional marginal distributions $F_{Y_{it}|\mathcal{F}_{t-1}}(y)$. 

Using the definition of a conditional copula one can write
\begin{equation}
\label{eq:conditionalcopula}
F_{Y_t|\mathcal{F}_{t-1}}(y_1, \dots, y_N) = C_{U_t|\mathcal{F}_{t-1}}(F_{Y_1t|\mathcal{F}_{t-1}}(y_1), \dots, F_{Y_Nt|\mathcal{F}_{t-1}}(y_N)).
\end{equation}
To have a valid conditional multivariate distribution, the conditioning set must be the same for the marginal distributions and the copula \parencite[p. 772]{Patton2009}. 

For the following sections we built on a semiparametric copula-based multivariate dynamic model as described in \textcite[126-129]{Chen2006}. The vector of conditional means and variances of $\bm{Y}_t|\mathcal{F}_{t-1}$ are estimated parametrically. The joint distribution of the estimated standardized residuals is then modelled using non-parametric estimates of the marginal distributions and a parametric time-invariant copula.

Thus, in this model three estimation stages have to be performed: First, the parameters of the marginal models for the conditional time varying means and variances. Second, the estimation of the marginal distribution of the standardized residuals. This is done using the empirical distribution function. Third, the parameters of the copula function for the standardized residuals. 
  
If we denote the parametrized conditional mean of a single variable as $\mu_{it} = E(Y_{it}|\mathcal{F}_{t-1}; \bm{\phi})$ and the parametrized conditional standard deviation as $\sigma_{it} = \sqrt{V(Y_{it}|\mathcal{F}_{t-1}; \bm{\phi})}$ we can write the multivariate time series as:
\begin{equation}
\label{eq:model}
\bm{Y}_t = \bm{\mu}_t + \bm{\sigma}_t\bm{\eta}_t
\end{equation}
with $\bm{\sigma}_t = \diag(\sigma_{1t}, \dots, \sigma_{Nt})$. The standardized innovations $\bm{\eta}_t = (\eta_{1t}, \dots, \eta_{Nt})'$ are assumed to be iid distributed and independent of past information. It follows that $F_{\bm{\eta}_{t}} = F_{\bm{\eta}} \forall t = 1, \dots, T$. Hence, for this model, the notion of a conditional copula as given by \eqref{eq:conditionalcopula} is not used. Instead the joint cdf of the residuals can be written as:
\begin{equation}
F_{\bm{\eta}}(u_1, \dots, u_t) =  C_U(F_{\eta_1}(u_1), \dots, F_{\eta_N}(u_N); \bm{\theta}),
\end{equation}
where the marginal distribution are estimated with the empirical distribution function:
\begin{equation}
\hat{F}_{\eta_i}(u) = \frac{1}{T+1}\sum_{t = 1}^{T}\bm{1}(\hat{\eta}_{it} \leq u).
\end{equation}



## Simulated methods of moments estimation for factor copulas
\label{s:SMM}



Estimation methods used for copula models depends on the degree of parametrization: For fully parametrized models for the copula and the marginal distributions maximum likelihood or multi-stage maximum likelihood is used. But one can also non parametrically estimate the marginal distributions and combine them with a parametric copula as shown in the previous section. 

In this case, pseudo-maximum likelihood estimation can be used. If a closed form functional relation of spearman's rho or kendall's thau to the copula parameters is available, one can also solve the system directly by using a method of moments approach. For the SMM approach, the population based statistics are replaced by their sample counterparts (inversion method). 

For the factor copula model a closed form one to one mapping of the copula's parameters $\theta$ to measures of dependency as defined in \eqref{eq:kendalls} - \eqref{eq:taildependency} is not available in general. If it were available, methods of moments or generalized methods of moments (if the number of moment conditions is larger than the number of parameters) could be applied \parencite[p. 689f]{Patton2013}. 

Instead one can use a set of scale-invariant empirical dependence measures calculated with simulations from the artificial variables $\bm{X}$ and compare them to the dependence measures obtained from the observable data $\bm{Y}$. Minimizing the weighted squared difference of the two dependency vectors yields an estimator for $\theta$. 

Formally, the estimator is given by 
\begin{equation}
\label{eq:SMM}
\hat{\bm{\theta}} = \arg \min Q(\bm{\theta}) = \arg \min \bm{g(\theta)'\hat{W}g(\theta)}
\end{equation} 
with
\begin{equation}
\label{eq:momentcondition}
\bm{g(\theta)} = \hat{\bm{m}} - \tilde{\bm{m}}(\bm{\theta}),
\end{equation}
where $\hat{\bm{m}}$ and $\tilde{\bm{m}}$ are vectors of empirical dependency measures computed from the observable and the simulated data respectively. 

For the dependency measures one uses the empircal counterparts of Spearmans Rho as in \ref{eq:spearman} and lower and upper tail dependency as stated in \ref{eq:taildependency}. Other invariant measures which only dependend on the copula, e.g. Kendall's Tau, can also be used. 

The vector $\bm{m}$ consists of a vectorized set of pairwise dependence vectors $\bm{\delta}_{i, j}$, where $i,j \in \{1, \dots, N\}, j < i$.   

For an unrestrictive model there exist $0.5*N*(N-1)$ vectors of dependencies, one for each unique pair of variables.  For the simpler equidependence model, which assumes the same factor loading for all variables, one can average over all vectors such that we can write $\bar{\bm{\delta}} = \frac{2}{N*(N-1)}\sum_{i = 1}^{N-1}\sum_{j = i+1}^{N} \bm{\delta}_{i, j}$. This results in a single vector of dependencies for $\bm{m}$. 

For the bloc-equidependence model the final number of dependency vectors is $M$ since one can average over all intra- and intergroup dependencies. For each group $s = 1, \dots, M$, we can write 

\begin{equation}
\bar{\bm{\delta}}_s =  \frac{1}{M}(\underbrace{\sum_{r = 1, r \neq s}^{M} \frac{1}{k_sk_r}\sum_{i = 1}^{k_s}\sum_{j = 1}^{k_r}  \bm{\delta}_{is, jr}}_{\text{intergroup dependencies}} + \underbrace{\frac{2}{k_s(k_s-1)}\sum_{i = k_s}^{k_s-1}\sum_{j = i+1}^{k_s} \bm{\delta}_{is, js}}_{\text{intragroup dependencies}}), s, r = 1, \dots, M.
\end{equation}

\textcite[p. 691ff]{Patton2013} showed that given some assumptions the SMM estimator is weakly consistent and asymptotically normal distributed. The assumptions ensure that for both the iid and the time series case the sample dependency measures converge in probability to their theoretical population values. Further it is important to note, that it is assumed that the population version of the moment conditions \eqref{eq:momentcondition} is differentiable at the true parameter $\bm{\theta}_0$ while this is not true for the finite sample version. 

The convergence can be summarized as:
\begin{align}
\label{eq:asymptotic}
\begin{split}
\hat{\bm{\theta}} \sim N(\bm{\theta_0}, (\frac{1}{T}+\frac{1}{S})\bm{\Omega}) \text{ ~for~ }T, S \rightarrow \infty,
\end{split}
\end{align}
with covariance matrix $\bm{\Omega} = \bm{(G'WG)^{-1}G'W'\Sigma WG(G'WG)^{-1}}$, where $\bm{G} = \frac{\partial \bm{g(\theta)}}{\partial \bm{\theta}_0}$ is the Jacobian matrix of the first order derivitives of the moments function $\bm{g(\theta)}$ and $\bm{\Sigma}$ is the asymptotic variance of the moments estimator $\hat{\bm{m}}$.

The matrix $\bm{W}$ is a positive definite weight matrix and can be chosen to be the identity matrix. If the efficient weight matrix $\bm{W} = \bm{\Sigma}^{-1}$ is used the aymptotic variance simplifies to $\bm{\Omega} = \bm{(G'WG)^{-1}}$. 

It is crucial to note, that the standard error of the copula parameters $\hat{\bm{\theta}}$ is not affected by the error arriving from estimating $\hat{\bm{\phi}}$, which is the parameter vector for the marginal models for the conditional mean and variance of $\bm{Y}_t$ (see \eqref{eq:model}). The asymptotic results as they are state here are ony valid if the marginal distribution of the residuals is estimated non-parametically by using the empirical distribution function.

The asymptotic variance $\bm{\Sigma}$ can be estimated using an iid bootstrap procedure:

\begin{algorithm}
\KwIn{$\{\hat{\bm{\epsilon}}_t\}_{t = 1}^T$}
\KwOut{$\hat{\bm{\Sigma}}_{T,B}$}
$\hat{\bm{m}}_T \gets$ compute sample moments from residuals $\{\hat{\bm{\epsilon}}_t\}_{t = 1}^T$ \;
  \For{$b\gets1$ \KwTo $B$}{
    $\{\hat{\bm{\epsilon}}_t^{(b)}\}_{t = 1}^T \gets$ sample $T$ values with replacement from $\{\hat{\bm{\epsilon}}_t\}_{t = 1}^T$\;
$\hat{\bm{m}}_T^{(b)} \gets$ compute sample moments from bootstrap sample $\{\hat{\bm{\epsilon}}_t^{(b)}\}_{t = 1}^T$\;
    $\bm{g}_{T}^{(b)} \gets \hat{\bm{m}}_T^{(b)} - \hat{\bm{m}}_T$\;
    }
    $\hat{\bm{\Sigma}}_{T,B} \gets \frac{T}{B}\sum_{b = 1}^B(\bm{g}_{T,B}\bm{g}_{T,B}')$\;
    \KwRet $\hat{\bm{\Sigma}}_{T,B}$\;
  \caption[Bootstrap procedure for the estimation of $\hat{\bm{\Sigma}}_{T,B}$.]{Bootstrap procedure for the estimation of $\hat{\bm{\Sigma}}_{T,B}$: The asymptotic covariance matrix of the moment condition  for the residual data.}
\label{alg:sigma}
\end{algorithm}

The derivative $\bm{G}$ can be estimated using a numerical approximation arround the parameter estimator $\hat{\bm{\theta}}$. For the $k$th column we can write:
$$
\hat{\bm{G}}_{T, S, k} = \frac{\bm{g}_{T, S}(\hat{\bm{\theta}}+\bm{e}_k\epsilon_{T, S}) - \bm{g}_{T, S}(\hat{\bm{\theta}}-\bm{e}_k\epsilon_{T, S})}{2\epsilon_{T, S}},
$$
where $\bm{e}$ is the $k$th unit vector and $\epsilon_{T, S}$ the step size which is usually set to $\epsilon_{T, S} = 0.1$. 


## Structural break test for factor copulas
\label{s:breaktest}

\textcite{Manner2012} gives some  overview over time varying copulas. 

In the following section a structural break test based on \textcite{Wied2017} for a change of the copula parameters in \eqref{eq:model} is presented. Note that it is assumed that the functional form of the copula is time invariant while the copula's parameters $\theta_t$ for $t = 1, \dots, T$ can vary over time. The model presented in \ref{s:dynamiccopula} allows for a wide variety of parametrization and copula functions. Here, we focus on the factor copula model and the SMM estimation procedure as presented in the previous sections. 

The test is based on recursive estimations of the copula model at each timepoint $t \in \epsilon 1, \dots, T$. $\theta_t$ is the estimated parameter using the data up to point $t$. The strictly positive trimming parameter $\epsilon$ has to be chosen such that the model parameters don't fluctutate too much just because of small sample sizes. The null hyopthesis states that the estimator is time invariant while the alternative indicates at least one significant break point in time. 
$$
H_0: \theta_1 = \theta_2 = \dots = \theta_T \hspace{2em} H_1: \theta_t \neq \theta_{t+1} \text{ for some } t = \{1, \dots, T\}
$$

The the test statistics is the maximum of the scaled squared distance between the full model and the recursive estimator:
\begin{equation}
P = \underset{\epsilon T \leq t \leq T}{\max} \frac{t}{T}^2T(\btheta_{t, S}- \btheta_{T, S})'(\btheta_{t, S}- \btheta_{T, S}).
\end{equation}

An alternative test statistic is solely based on the moment generating functions and does not involve the estimation of a copula model:
\begin{equation}
M = \underset{\epsilon T \leq t \leq T}{\max} (\frac{t}{T})^2T(\bm{\hat{m}}_t - \bm{\hat{m}}_T)'(\bm{\hat{m}}_t - \bm{\hat{m}}_T).
\end{equation}

If the test statistic is larger than some critical value, the time $t$ where the test statistic occurs can be interpreted as the detected breakpoint.

Under the null hypothesis and given some assumptions which are similar in spirit to the assumptions for the asymptotic distribtion as in \eqref{eq:asymptotic} the test statistics converges in distribution to 
\begin{equation}
P \overset{d}{\rightarrow} \underset{\epsilon T \leq t \leq T}{\max} (\bm{A}^*(t) - s\bm{A}^*(T))'(\bm{A}^*(t) - s\bm{A}^*(T)), 
\end{equation}
with $\bm{A}^*(t) = (G'WG)^{-1}G'W(A(t) - \frac{s}{\sqrt{k}}A(1))$ and $T, S \rightarrow \infty, \frac{S}{T} \rightarrow k \text{ or } \frac{S}{T} \rightarrow \infty$.

The distribution under the null hypothesis can be estimated using the following bootstrap procedure:

\begin{algorithm}
\KwIn{$\hat{\bm{\epsilon}}_t; \alpha$}
\KwOut{$K$}
  compute full sample moments $\hat{\bm{m}}_T$\;
  \eIf{copula based test}{
      $L \gets (\hat{G}'\hat{W}\hat{G})^{-1}\hat{G}'$\;
  }{
      $L \gets  1$\;
  }
  \For{$b\gets1$ \KwTo $B$}{
    generate bootstrap sample $\{\hat{\bm{\epsilon}}_t^{(b)}\}_{t = 1}^T$\;
    \For{$t\gets T$ \KwTo $\epsilon T$}{
      compute recursive bootstrap sample moments $\hat{\bm{m}}_t^{(b)}$\;
      $A^{(b)*}_t \gets L\frac{t}{T}\sqrt{T}(\hat{\bm{m}}_t^{(b)} - \hat{\bm{m}}_T)$\;
      $K^{(b)}_t \gets (A^{(b)*}_t - \frac{t}{T}A^{(b)*}_T)'(A^{(b)*}_t - \frac{t}{T}A^{(b)*}_T)$\;
    }
    calculate the maximum $K^{(b)}$ over all $K^{(b)}_t$\;
  }
  calculate the $1-\alpha$ sample quantile $K$ from all $K^{(b)}$\;
  \KwRet K;
  \caption{Bootstrap procedure for the estimation of the critical value for the structural break test.}
  \label{alg:breaktest}
\end{algorithm}


# \emph{factorcopula} - an R package for simulation and estimation of factor copulas
\label{s:factorcopulapackage}

In this chapter we present the implementation of the previous methods. Using the programming language \emph{R}, the functions are bundled in an R-package such that the methods can be easily installed and distributed. The validity of the package is tested in two simulation studies. 

The package consists of a set of high level functions which can be used to construct, simulate and fit various factor copula models. The specification of the factor copula model is handled by the functions `config_factor`, `config_error` and `config_beta`. The two functions `fc_create` and `fc_fit` can be used to either simulate values from a factor copula or to fit a model to a dataset. For conducting the break test as described in section \ref{s:breaktest}, the library offers the functions `fc_critval`, `fc_mstat` and `fc_pstat`. The former simulates critical values for either the moments or copula based test. The latter two calculate the recursive test statistics. A matrix of recursive $\theta$ estimates is needed for the calculation of the copula based test statistic. It can be obtained by recursively applying `fc_fit` to the data.  

## Copula specification and simulation

With the functions `config_factor`, `config_error` and `config_beta`, the user can define the distribution of the latent variables $\bm{Z}$, the error term $\bm{\epsilon}$ and the matrix of factor loadings $\bm{\beta}$. For the specification of the distributions, the function name of any available random number generator can be used. Additional arguments such as distributional parameters can be declared in a named list. 

In the following example, a factor copula with three latent variables is defined. The first factor is skewed-t distributed, the second and third are standard normal distributed. The error term is t distributed with 4 degrees of freedom. The distributional parameters `df` and `lambda` of the skewed-t distribution are free parameters of the model.  

```{r}
library(factorcopula)
Z <- config_factor(rst = list(nu = df, lambda = lambda), 
                   rnorm = list(),
                   rnorm = list(),
                   par = c("df", "lambda"))
eps <- config_error(rt = list(df = 4))
```

Distributional parameters can either be fixed or passed as non-evaluated expressions. To distinguish free model parameters from fixed distributional parameters, an additional character vector with the name of the model parameters has to be passed to `config_factor` and `config_error`. 

The random number generating functions must have an additional argument `n` which defines the number of observations to be simulated. This argument should not be set explicetly since the number of simulations is controlled by the value $S$ later. 

For the factor loadings the user can either manually construct a character matrix of parameters or one can use the function `config_beta`. Given a vector `k` and the number of latent variables $K$ this functions constructs a suitable character matrix of zeros and some parameter names. 

The vector $k$ is of length $N$ and defines the group for each observable variable. Therefore, an equidependence model can be specified with $k_N = (1, 1, \dots, 1)$, an unrestrictive model with $k_N = (1, \dots, N)$ and a bloc-equidependence model with $k_N = (1, 1, \dots, 2, 2, \dots, M,M, \dots)$, where $M$ is the number of groups. 

Continuing the example, a bloc-equidependence model with $K=3$, $N = 6$, $k_1 = k_2 = 3$ and $M = 2$ is constructed. Together with the specification of the latent variables this results in a bloc-equidependence model with a skewed-t distribution for the common factor and a standard normal distribution for each of the two group specific factors. 
```{r}
k <- c(1, 1, 1, 2, 2, 2)
beta <- config_beta(k, 3)
```

The function `fc_create` returns itself a random number generating function. To simulate values from it, the user has to specify a \emph{named} vector $\theta$ of parameters, the number of simulations $S$ and an optional random seed. Fixing the seed at some value always gives the same simulated numbers. The vector `theta` must be a named vector for which the names correspond to the model parameters specified during the configuration of the model. 

In the example, $S = 10$ random values from the copula model are simulated. The loadings for the common factor are $\beta_1 = \beta_2 = 1$ and the group specific loadings $\beta_3$ and $\beta_4$ are set to zero. 

```{r, eval = FALSE}
copfun <- fc_create(Z, eps, beta)
theta <- c(beta1 = 1, beta2 = 1, beta3 = 0, beta4 = 0, 
           df = 10, lambda = -0.8)
U <- copfun(theta, S = 10)
```

The simulation of new values is a time consuming part. To avoid unnescessary calls to the underlying random number generators, the function remembers the state of the $Z$ and $\epsilon$ matrix over repeated function calls and only updates the values if nescessary. Thus if neither the seed nor the distributional parameters in `theta` change, the function uses the same random values from a previous call. This reduces the computation time at the cost of a higher memory consumption.\footnote{A simple local comparisson of an optimized and unoptimized function showed that given a fixed seed and no change in distributional parameters, the opimized version runs five times faster for $S = 10000$ and three times faster for $S = 100000$.}


## Optimization strategy 

While the function `fc_create` is used to simulate values from a factor copula model given a parameter vector $\theta$, the function `fc_fit` estimates $\theta$ via SMM as described in section \ref{s:SMM}. Given a copula specification and some observable data the function minimizes the weighted squared distance between the moments based on simulated data and the moments based on observable data as presented in equation \eqref{eq:SMM}. 

Optionally, the function also estimates the standard errors by impementing the bootstrap algorithm [\ref{alg:sigma}]. The weight matrix $W$ is always set to the identity matrix, since previous studies showed no significant improvement when the efficient weight matrix $W  = \hat{\bm{\Sigma}}_{T,B}^{-1}$ is used \parencite[p. 694]{Patton2013}. 

As a backend, the method builts on top the `NLopt` optimization library which implements various algorithms for global, local or derivative-free optimization (see \textcite{nlopt2018} and \textcite{nloptr2014}). The choice of the optimization algorithms and the stopping criteria can be altered by the user via the arguments `control.first.stage` and `control.second.stage`. The authors of the library recommend a two step optimization procedure for global optimization: First, a global optimizer should be used to approximates the optimal region in which the global optimum lies. Second, a local derivative free optimizer can then be applied using the approximated solution from the first stage. 

By default, `fc_fit` uses the \emph{Multi-Level Single-Linkage} algorithm in the first step. The algorithm creates a sequence of optimal distributed starting values which are then passed to a local optimizer \parencite{Kucherenko2005}. For the local optimizer and the second stage, the \emph{Subplex} algorithm is used, which is based on the popula Nelder-Mead Simplex procedure \parencite{Rowan1990}. 

During the optimization process, many values from the factor copula model are simulated. To avoid numerical instablities, the random seed is kept fixed \parencite[app. to][p. 12f]{Patton2013}. As described in the previous section, the memory functionability of `fc_create` can save some computational costs, since redraws from the distributions are avoided if the distributional parameter don't change. This can improve the overall performance of the optimizataion process. Especially, if only the factor loadings $\bm{\beta}$ are optimized. In this case, the random number generators are only called once at the beginning of the optimization. 


## Simulation study

To illustrate the discussed methods and the validity and performance of the package, two simulation studies are performed: First, an equidependence factor copula model with varying dimensions and sample sizes is estimated repeatedly to show the consistency of the SMM procedure. Second, both the moments and copula based structural break test are applied once to simulated data from a bloc-equidependence model.

The DGP of the first study is based on a simple equidependence model with standard-normal marginal distributions, one skew-t distributed latent variable and a t-distributed error term. The copula model produces strong assymetric tail-dependencies. This is achieved by fixing the degrees of freedom at $df = 4$ and the skeweness at $\lambda = -0.8$. The single factor loading is set to $\beta = 1.5$. The only free parameter to be estimated is the factor loading. Thus we can write $\theta = \beta$. The model equations can be written as:

\begin{equation}
\begin{aligned}
Y_i &\sim N(0, 1) ~\forall~ i = 1, \dots, N\\
Z &\sim \text{skew-t}(df = 4, \lambda = -0.8) \\
\epsilon &\sim \text{t}(df = 4)\\
X_i &= \beta Z + \epsilon\\ 
\end{aligned}
\end{equation}

We repeat the simulation and estimation of $\beta$ over a grid of different values for the number of variables $N$ and the sample size $T$. Each simulation consists of $C = 1000$ Monte-Carlo replications. The number of simulations in the SMM is set to $S = 25000$. Finally, we get a vector of estimates $\hat{\beta}_{t, n, c}$ with $t \in (100, 1000, 10000), n \in (2, 3, 10), c \in (1, \dots, C)$. 

\begin{figure}[h]
	\includegraphics[width=\textwidth]{./figures/fig2}
	\caption[Monte-Carlo density estimators for $\hat{\beta}$]{Approximated Monte-Carlo density estimators for $\hat{\beta}$ of an equidependence skew t - t factor copula model with $\beta = 1.5$, $S = 25000$, standard normal distributed marginals and different values for $N$ and $T$. Each simulation is based on $1000$ Monte-Carlo replications.}	
	\label{f:montecarlo}  
\end{figure}

Figure \ref{f:montecarlo} shows the results for the first study. For each combination of $N$ and $T$ the kernel density estimator over all Monte-Carlo simulations is plotted. The bias $\hat{b}_{t, n} = \frac{1}{C}\sum_{c = 1}^{C}  \hat{\beta}_{t, n, c} - \beta$ and the standard deviation is printed in the top left corner. 

The density mass centers arround the true value of $\beta = 1.5$. For all simulations the bias is zero or close to it. As the sample size increases, the deviation gets smaller and the SMM estimator converges to the true parameter. But a small sample size clearly gives unreliable results. Having more variables improves the quality of the estimator. This is due to the fact that the number of latent variables and parameters is constant and does not increase with the number of dimensions. Thus, with larger $N$ more information is available to estimate the single parameter.

For the second study, a more sophisticated model is presented to illustrate the effectiveness of the approach even for high dimensional problems and complicated dependence structures. Analogous to the empirical examples in \textcite{Wied2017} and \textcite{Patton2017} the DGP is based on a \emph{bloc-equidependence} model as described at the end of section \ref{s:factorcopula}. The model consists of $N = 21$ standard normal distributed variables which are partitioned in $M = 3$ groups of equal size. For the factor copula, the single common latent factor is aggain skew-t distributed with strong assymetric tail-dependency. The three group specific latent factors and the error term are t-distributed. Due to the bloc-equidependence structure, the number of factor loadings reduces from $0.5*N*(N-1) = 210$ to just $2M = 6$. All distributional parameters are fixed. Therefore, only the factor loadings are about to be estimated such that $\theta = (\beta_1, \dots, \beta_6)$. 

We chose $T = 1500$, $S = 25 \times T$ and a breakpoint of the copula parameters at $t = 1000$. Before the break, $\theta_{pre} = (0, 1, 1, 0, 1, 1)$ and after the break $\theta_{post} = (1.5, 1, 1, 1.5, 1, 1)$. Thus, only the intra- and interdependence for the first group increases from $0$ to $1.5$ while the remaining loadings stay constant.

The test statistics and critical values are based on three different recursive calculations over a range from $t = 300$ to $T = 1500$: First, all factor loadings are estimated. Second, a subset of $\theta$ is estimated recursively while fixing the common and group specific factor for the first group at the full sample estimates. Hence, for this calculation $\theta = (0, \beta_2, \beta_3, 0.88, \beta_5, \beta_6)$. Finally, to perform the moments based test, the empirical dependence vectors are calculated wihtout estimating the copula parameters.

We expect, that a breakpoint is detected arround $t = 1000$ only for the first and third recursive calculation. For the second calculation no breakpoint should be detected since the factor loadings of the second and third group are not affected by the simulated structural break.  

Figure \ref{f:breaktest} shows the test statistics of the three different break tests for a single recursive run of the simulation for $t = 300, \dots, 1500$. The horizontal solid line indicates the critical value. Each critical value is calculated using $B = 2000$ bootstrap samples as described in algorithm [\ref{alg:breaktest}]. The vertical line indicates the theoretical breakpoint at $t = 1000$.  

\begin{figure}[H]
	\includegraphics[width = \textwidth]{./figures/fig3}
	\caption[Illustration of a structural break test for a bloc-equidependence model]{Illustration of a structural break test for a bloc-equidependence model with $N = 21$ and $3$ groups of equal size. The theoretical breakpoint is at $t = 1000$ and is modelled as a change of the intra- and interdependency of the first group from $0$ to $1.5$. The first panel shows the copula based test based on all groups. The second panel the copula based test for only the second, and third group which are not affected by the structural break. The lower panel shows the moment based test.}	
	\label{f:breaktest}  
\end{figure}

As expected, both the full copula and the moments based test statistics reach their maximum arround $t = 1000$. In both cases the maximum lies above the critical value. The CV for the moments based test is relatively far away from the maximum while this is not true for the full copula based test. This could indicate that the moments based test is less sensitive while the copula based test is more restrictive. This observation is also confirmed in the empirical example in the following chapter. The copula based test statistics for the second and third group are clearly below their critical value. Hence, for this groups the null hypothesis of no parameter change cannot be rejected. 

If the parameter vector is large, recursively estimating the copula model is computationally quite challenging and often results in numerical instabilities. Therefore, the test statistic can be substantially distorted which results in extreme outliers above the CV. Therefore, we advise to always perform a manual graphical analysis of the final test statistics. In addition, clear outliers can be detected by performing a smoothing method on the test statistics (e.g. running medians or smoothing splines). 

Finally, table \ref{tab:simulation} shows the results for a copula model estimated on the full sample, before and after the theoretical breakpoint. For the model after the breakpoint the estimates are less precise due to the small sample size. This can also be seen in the relatively high value of the objective function $Q$.  

\begin{table}[H]
\centering
\begin{tabular}{rlll}
  \toprule
  coefficient & pre-break ($t \leq 1000$) & post-break ($t > 1000)$ & full sample \\
  \midrule
  $\beta_1$ & 0.00 (0.10) & 1.96 (0.69) & 0.00 (0.13) \\ 
  $\beta_2$ & 1.17 (0.37) & 0.88 (0.29) & 1.28 (0.30) \\ 
  $\beta_3$ & 0.96 (0.27) & 0.83 (0.30) & 0.99 (0.27) \\ 
  $\beta_4$ & 0.03 (0.15) & 1.55 (0.64) & 0.88 (0.15) \\ 
  $\beta_5$ & 0.92 (0.18) & 1.01 (0.32) & 0.92 (0.18) \\ 
  $\beta_6$ & 1.04 (0.20) & 1.13 (0.40) & 1.09 (0.20) \\
  \midrule
  $Q$ & 0.0008 & 0.0026 & 0.0025 \\ 
  $T$ & 1000 & 500 & 1500 \\
  $S$ & 37500 & 37500 & 37500 \\
  \bottomrule
\end{tabular}
\caption[Estimation results for the simulated model before and after the break and for the full span.]{Estimation results for the bloc-equidependence factor copula model before and after the breakpoint and for the full dataset. Standard errors in paranthesis (estimated with $ B = 2000$ bootstrap samples).}
\label{tab:simulation}
\end{table}



# Modelling topic dependencies over time with factor copulas

In this chapter we apply the previously discussed methods to a real aggregated dataset derived from the social media platform Facebook. First the dataset is presented. Second, we give a detailed description of the feature generation process and some descriptive overview. Third, the model setup is explaind and estimation results of various factor copula models applied to the residuals of the word frequencies are presented. Finally, we apply both the moments and copula based structural break test to the aggregated data.  


## The \emph{btw17} social media dataset

The raw data consists of social media posts published by public pages on Facebook between January 2014 and December 2017. Using the official list of the candidates for the Bundestag election in 2017 (btw17), the account for each of the politicians was manually researched. Only candidates from the six factions in the \emph{Bundestag} (CDU/CSU, SPD, Die Linke, Bündnis 90/ Die Grünen, AfD, FDP) are part of the study. Arround 84\% of all 2516 candidates have an account on Facebook \parencite[see][p. 16]{Stier2018}.

Due to API and privacy restrictions, only information from public pages can be accessed such that arround 52\% of the social media accounts could be considered for the data collection. In addition to the candiates pages, $113$ official pages from the political parties, both on the federal and regional level, were included. 

The data collection took place on several days between 2017-11-21 and 2018-02-06. The web-scraping software is built on top of the \emph{restfb} Java client library and makes calls to Facebook's official \emph{Graph API} \parencite{restfb2018}. The posts are stored in a document orientated database on cloud-servers located in Germany. Besides the actual message, they contain a timestamp, the user-id of the author and the number of likes and shares the post has received uppon collecting it from the API.

For this analysis, the data is restricted on textual posts only.\footnote{A post can also consist of a foto, an album or an event.}. This results in almost 664 thousand posts tagged with the party membership of their authors.

The first two panels of figure \ref{f:nPosts} show for each party the monthly number of active accounts and the monthly number of posts. An account was defined active if it has at least one post during the month considered. 

In early 2014 approximately 500 accounts were active. This number increased steadily to roughly 750 accounts in mid 2016. From then until the election in September 2017 the number increased rapidly to almost 1200 active accounts followed by a drop after the election. Since we only collected accounts from politicians which were candidates in the btw17 the sharp rise of active accounts can be explained by the fact that many politicians opened an account just for the election campaign. After the election many candidates closed their accounts due to a failure in the elections. A similar pattern can be observed for the monthly number of posts. 

\begin{figure}[H]
\flushleft \textbf{Accounts and posts by party over time}\\[1.5em]
	\includegraphics[width=\textwidth]{./figures/nPosts}
	\caption{Number of active accounts and number of posts per party and month. The bottom panel shows the number of posts matching the regular expression \texttt{flucht|fluecht}. The vertical line indicates the breakpoint detected by the moments based test.}	
	\label{f:nPosts}  
\end{figure}

The bottom panel of figure \ref{f:nPosts} shows the absolute number of monthly posts related to the topic "refugees". We define a post to be refugee related if it matches the regular expression \texttt{flucht|fluecht}.\footnote{Note that a posts's text was cleaned by removing links and stopwords, transforming umlauts and converting the text to lowercase letters.} For example, a match occurs if a post contains the words \emph{flüchtlingskrise} (refugee crisis), \emph{fluchtursachen} (causes of flight), \emph{flüchten} (to flee) or \emph{flüchtlingsheime} (refugee hostels). A sharp rise of refugee related post can be observed in autumn and winter of 2015 and 2016. During this time many refugees entered Germany, fleeing from the war in Syria. This event is commonly labeled as the "german refugee crisis" in the media and the political discussions. 

Table \ref{t:facebook} summarizes the monthly numbers presented in figure \ref{fig:nPosts}. For each party, it shows the overall number of posts, accounts, aggregated likes and shares for posts and the within party share of refugee related posts from 2014-01-01 to 2017-12-31. Allthough the right and left wing parties \emph{AfD} and \emph{Linke} have a relatively small number of accounts and posts they generate by far the greatest number of attention in terms of likes and shares. The social democratics party \emph{SPD} has over twice more posts than the \emph{AfD} but roughly generates only half of the likes and only a fifth of the shares. Looking at the share of refugee related posts one can see that both the far left and far right parties talk more about refugee related topics than the average.

\begin{table}[h]
\centering
\begin{tabular}{rccccc}
  \toprule
 party & posts & accounts & likes & shares & share of posts related to  \\ 
 & & &\multicolumn{2}{c} {(in Million)} &  "refugees" (in \%) \\ 
\cmidrule(r){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(l){6-6}
AfD & 74724 & 162 & 18.36 & 7.59 & 6.96 \\ 
  CDU/CSU & 169115 & 267 & 14.72 & 1.83 & 3.49 \\ 
  FDP & 71083 & 201 & 6.32 & 0.77 & 2.39 \\ 
  Grüne & 67188 & 139 & 4.03 & 1.28 & 4.62 \\ 
  Linke & 84723 & 158 & 16.03 & 4.23 & 6.23 \\ 
  SPD & 196805 & 290 & 10.11 & 1.57 & 3.66 \\
\cmidrule(r){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(l){6-6}
  All parties & \numprint{663638} & 1217 & 69.57 & 17.27 & 4.28\\
  \bottomrule
\end{tabular}
\caption{Overall number of posts, active accounts, likes and shares over the observation period from "2014-01-01" - "2017-12-31". The last two columns show the fraction of posts matching the regular expression \texttt{flucht|fluecht}.}
\label{t:facebook}
\end{table}

## Data processing and descriptive analysis

For each party we count the daily number of refugee related posts and divide it by the overall daily number of posts from this party. Thus, for each day, we get a relative within-party frequency of refugee related posts. The final dataset consists of $T = 1461$ rows and a column for each of the six parties. We use this statistic to approximate the importance of refugee related topics in the political discourse for this party. For example, a value close to $0.5$ indicates that half of the political discourse from this party is refugee related. 

A pairwise positive dependence between all parties could indicate that refugee related topics are equally relevant for all parties and that the discussions are driven by the same events. Whereas no dependence could indicate, that the importance is mainly party-specific and independent from outside discussions. Tail dependencies could indicate that only in the time of disruptive external events the importance of refugee related topics is equally high for alle parties. 

In the spirit of the copula time series model as discussed in \ref{s:dynamiccopula} we first estimate univariate models for each of the time series. The factor copula dependency analysis is then performed on the residual information. By removing the time dependent conditional mean and standard deviation from the observations, it can then be assumed that the residuals are time invariant. The empirical distribution function is used to model the marginal distributions of the residuals.

We use a standard ARIMA-GARCH model for modelling the time series \parencite{Teraesvirta2009}. For the conditional variance we assume a GARCH(1, 1) process while specific ARIMA models are applied to model the conditional mean. The parts of the mean model are determined by running various models over a grid of different parameters. The model candidate with the lowest bayseian information criterion (BIC) is then chosen. Table \ref{tab:ARIMA} summarizes the final parameter values for the six time series model applied to the relative daily frequencies. 

\begin{table}[H]
\centering
\begin{tabular}{rcccccc}
  \toprule
  Party & AfD & CDU/CSU & FDP & Grüne & Linke & SPD \\
  \midrule
  AR & 4 & 1 & 2 & 3 & 2 & 1\\
  I & 1 & 1 & 1 & 1 & 1 & 1\\
  MA & 1 & 2 & 1 & 3 & 2 & 1\\
  \bottomrule
\end{tabular}
\caption{ARIMA Model parameters for the ARIMA-GARCH(1,1) model used to estimate the standardized residuals.}
\label{tab:ARIMA}
\end{table}

Figure \ref{f:residuals} shows the time series of relative frequencies and the residuals for each of the six parties after applying the ARIMA-GARCH(1, 1) models. Inline with the bottom panel of figure \ref{f:nPosts}, refugee related topics start to become important in midth 2015 with peaks of up to 40\% in late 2015. Espacially for the left and right wing parties, "Linke" and "AfD", the topic stays important even after the crisis. The ARIMA-GARCH model seem to correctly fit the time series but some extreme positive outliers are prevalent.

\begin{figure}[H]
	\subfloat[Relative daily within-party frequency of refugee related posts.]{\includegraphics[width = \textwidth]{./figures/flucht_observed}} \\
	\subfloat[Residuals of ARIMA-GARCH models applied to the time series.]{\includegraphics[width= \textwidth]{./figures/flucht_residuals}}
	\caption{}	
	\label{f:residuals}  
\end{figure}

Table \ref{t:correlations} shows the overall pairwise dependency measures used also for the SMM procedure. The dependencies are very weak to weak. 

\begin{table}[H]
\centering
\begin{tabular}{rccccc}
  \toprule
  & Rank - & \multicolumn{4}{c}{Quantile-dependence}\\
 Pairs &  correlation & 0.05 & 0.1 & 0.90 & 0.95 \\ 
  \cmidrule(r){1-1} \cmidrule(rl){2-2} \cmidrule(l){3-6}
AfD-CDU/CSU & 0.07 & 0.14 & 0.17 & 0.13 & 0.10 \\ 
  AfD-FDP & 0.08 & 0.10 & 0.14 & 0.13 & 0.07 \\ 
  AfD-Grüne & 0.03 & 0.07 & 0.16 & 0.09 & 0.10 \\ 
  AfD-Linke & 0.08 & 0.07 & 0.15 & 0.19 & 0.08 \\ 
  AfD-SPD & 0.09 & 0.11 & 0.16 & 0.16 & 0.08 \\ 
  CDU/CSU-FDP & 0.17 & 0.16 & 0.25 & 0.18 & 0.14 \\ 
  CDU/CSU-Grüne & 0.13 & 0.11 & 0.20 & 0.14 & 0.07 \\ 
  CDU/CSU-Linke & 0.10 & 0.10 & 0.18 & 0.16 & 0.14 \\ 
  CDU/CSU-SPD & 0.20 & 0.22 & 0.22 & 0.19 & 0.14 \\ 
  FDP-Grüne & 0.10 & 0.08 & 0.19 & 0.15 & 0.10 \\ 
  FDP-Linke & 0.10 & 0.10 & 0.16 & 0.17 & 0.10 \\ 
  FDP-SPD & 0.12 & 0.19 & 0.23 & 0.18 & 0.07 \\ 
  Grüne-Linke & 0.09 & 0.07 & 0.12 & 0.19 & 0.08 \\ 
  Grüne-SPD & 0.12 & 0.14 & 0.21 & 0.16 & 0.14 \\ 
  Linke-SPD & 0.15 & 0.16 & 0.16 & 0.23 & 0.21 \\ 
  \cmidrule(r){1-1} \cmidrule(rl){2-2} \cmidrule(l){3-6}
  Average & 0.11 & 0.12 & 0.18 & 0.16 & 0.11 \\ 
   \bottomrule
\end{tabular}
\caption{Pairwise empirical dependencies for the six parties.}
\label{t:correlations}
\end{table}

 

## Results

First, various factor copula models are fitted to the complete dataset. Second, the best model is chosen and recurseviley applied to the data while fixing the distributional parameters at the complete sample estimates. A moments and copula based break test is performed. Finally, copula models are estimated for the pre-break and post-break period.

To dertermine the number of latent factors $K$, we analyze the ordered eigenvalues of the residual rank-correlation matrix, as described in \textcite[p. 148]{Patton2017}. One eigenvalue is above the threshold (see \ref{f:eigenvalues}). Thus, the analysis is restricted to factor copula models with one latent factor. 

In figure \ref{f:onefactor} we summarize the results of applying various one-factor copula models to the complete sample of residuals. A restrictive equidependence and an unrestrictive model is performed. 


\begin{table}[H]
\centering
\begin{tabular}{rllllll}
  \toprule
  & \multicolumn{3}{c}{Equidependence} & \multicolumn{3}{c}{Unrestrictive} \\ 
  & norm-norm & t-t & skewt-t & norm-norm & t-t & skewt-t\\
  \cmidrule(r){2-4} \cmidrule(l){5-7}
  $\beta_1$ & 0.43 & 0.42 & 0.42 & 0.23 & 0.23 & 0.25 \\ 
  $\beta_2$ &      - &      - &      - & 0.62 & 0.52 & 0.63 \\ 
  $\beta_3$ &      - &      - &      - & 0.42 & 0.44 & 0.43 \\ 
  $\beta_4$ &      - &      - &      - & 0.32 & 0.34 & 0.33 \\ 
  $\beta_5$ &      - &      - &      - & 0.37 & 0.41 & 0.35 \\ 
  $\beta_6$ &      - &      - &      - & 0.63 & 0.60 & 0.67 \\ 
  \cmidrule(r){2-4} \cmidrule(l){5-7}
   $df$ & - & 99 & 96 &  - & 58 & 38 \\ 
  $\lambda$ & - & - & -0.29 & - & - & -0.36 \\ 
  \cmidrule(r){2-4} \cmidrule(l){5-7}
  $Q$ & 0.0036 & 0.0034 & 0.0031 & 0.1081 & 0.0979 &  0.0903\\ 
  \bottomrule
\end{tabular}
\caption{Estimation results for different one-factor copula specifications.}
\label{f:onefactor}
\end{table}

Using the one-factor skew-t equidepndence model as 

\begin{table}[H]
\centering
\begin{tabular}{rccc}
  \toprule
  & \multicolumn{2}{c}{Moments based test}& Copula based test \\ 
  & unrestrictive & restrictive & restrictive \\
  \cmidrule(r){1-1} \cmidrule(lr){2-3} \cmidrule(l){4-4}
  test statistics & 6.05 &            116.95  &   1.51 \\ 
  95\%-CV  &      1.98 &       61.90 &      1.67 (p-value: 0.0685)  \\ 
  breakpoint      &  2015-09-18    &  2015-09-18  &      2015-03-13 \\ 
  \bottomrule
\end{tabular}
\caption{Summary of break point detection methods for the social media dataset.}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{rlll}
  \toprule
  coefficient & before ($t \leq 626$) & after ($t > 626)$ & full model \\
  \midrule
  $\beta_1$ & 0.39 (0.03) & 0.41 (0.03) & 0.42 (0.03) \\ 
  $\lambda$ & -0.55 (0.75) & -0.33 (0.54) & -0.29 (0.37) \\
  \midrule
  $Q$ &  0.0029&  0.0023 & 0.0031 \\ 
  $T$ & 626 &  835 & 1461 \\
  $S$ & 36525 & 36525 & 36525 \\
  \bottomrule
\end{tabular}
\caption{Estimation results for the btw17 dataset. A bloc-equidependence factor copula model is estimated before and after the breakpoint at $t = 626$. Standard errors in paranthesis (estimated with $ B = 2000$ bootstrap samples). Degrees of freedom were fixed at the full model estimate of $df = 96$. }
\end{table}

# Discussion


To get better results much care has to be put into the optimiaztion algorithm and the numerical procedures. 
\textcite{Frazier2017} proposes ways to use derivative based optimizataion procedures in cases of SMM where the objective is discontinuty. 

\textcite{Embrechts2009} and \textcite{Mikosch2006} critizes the "hype" arround copulas. 





\newpage
\appendix

# Appendix

## Notes on data and source code access

An online version of this thesis is publicly available as a git-repository under https://github.com/bonartm/factorcopula-thesis. The repository contains notes on how to install all dependencies. The source code files for the analyses are located online in the `source` folder. 

Due to data restrictions by Facebook it is not possible to publish the original dataset of Facebook posts. However, the residuals of the ARIMA-GARCH models which were applied to the aggregated Facebook data are located online at `data/topics_residuals.rds`. 

The methods for the simulation and estimation of factor copula models and the break test are not part of this repository. Instead, they are available via the R-package `factorcopula`. The package can be installed from Github. Further notes can be found under: https://github.com/bonartm/factorcopula (See also chapter \ref{s:factorcopulapackage}). 

For almost all estimation procedures, the HPC cluster of the Universiy of Cologne was utlizied \parencite{cheops2018}. To simplify the workflow we wrote the R-package `cheopsr` which allows the execution of job scripts from within the local R environment. The package is available online at https://github.com/bonartm/cheopsr. To run the package, a Unix-like system and accesss rights to the HPC cluster are obligatory. 

## Additional figures

\begin{figure}[H]
	\includegraphics[width=0.95\textwidth]{./figures/pairwise_scatter}
	\caption{Pairwise scatterplot of the estimated residuals.}	
	\label{f:pairwise_scatter}  
\end{figure}

\begin{figure}[H]
	\includegraphics[width=0.95\textwidth]{./figures/eigenvalues}
	\caption{Scree-plot of ranked eigenvalues based on the pairwise rank-correlation matrix.}	
	\label{f:eigenvalues}  
\end{figure}










