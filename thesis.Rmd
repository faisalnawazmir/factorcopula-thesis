---
title: Testing for Structural Breaks in Factor Copula Models - Implementation and Application in Social Media Topic Analysis
author: Malte Bonart
type: Master thesis
logo: "UoC-Logo.eps"
institue: "Submitted for the Master Examination in Economics at the Faculty of Management, Economics and Social Sciences of the University of Cologne in June 2018."
supervisor: "Prof.Dr. Dominik Wied, Florian Stark"
bibliography: "./literature.bib"
output: 
  pdf_document:
    highlight: default
    #latex_engine: xelatex
    latex_engine: pdflatex
    template: thesis_template.tex
    citation_package: biblatex
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "\\textwidth")
#install.packages("devtools")
#devtools::install_github("bonartm/factorcopula")
```

# Introduction
\label{s:introduction}

Many multivariate statistical models are based on the assumption that the variables follow a multivariate Gaussian distribution. Here, the natural measure of dependence is the covariance or correlation matrix between the variables \parencite[p. 25]{Joe2015}. But using simple correlations as the only dependence measure can be misleading. In many cases real world data does not follow a Gaussian distribution and shows other distributional features, such as skewness or heavy tails. Correlation requires that the variance of the marginal distribution is finite. This requirement can be problematic when dealing with heavy tailed distributions. Furthermore, correlation is only a measure of linear dependence and is not invariant under monotonic transformations of the variables \parencite[6-8]{Embrechts2002}. For instance applying the strictly increasing logarithmic function to the variables usually results in a different correlation matrix.

Due to these shortcomings of the ordinary linear correlation coefficient, other and more flexible methods for describing the dependence structure between non Gaussian variables have been developed. In this area of research, models based on so called copula functions became increasingly popular since the \nth{1990} \parencite[p. 1]{Nelsen1999}. Especially in the last decade, an exponentially increasing boom in the research activity can be observed: Up to the year 1999, only 38 publications with the topics \emph{copula} and \emph{dependence} have been published in peer-reviewed journals. From 2000 to 2008 334 copula related publications are listed and for the years 2009 to 2017 the number of publications increased to 2048.\footnote{The numbers are taken from the \emph{Web of Science Core Collection} for the search term "\texttt{TOPIC: (copula) AND TOPIC: (dependence)}".} 

Applications of copula based models can be found in many applied disciplines and one of its first usage was in the field of survival analysis in bio-statistics \parencite{Fermanian2017}. But it recent popularity is mainly driven by applications in finance and insurance science, especially in quantitative risk management, e.g. for estimating the Value at Risk measure of a portfolio \parencite[644]{Embrechts2009}.

At first, any valid multivariate distribution function for which its marginal distributions have a uniform distribution on the interval $[0, 1]$ can be called a copula function \parencite[p. 1]{Nelsen1999}. Its popularity is based on the results of a theorem by \textcite{Sklar1959}: It implies that multivariate distributions can be constructed by separately specifying the marginal distributions of the random variables and by defining the dependence structure among the variables. The dependence structure is described with a copula function. It links the multivariate distribution function to its marginal distributions. In case of continuous multivariate distributions the copula function is uniquely defined. In contrast to the linear correlation coefficient, scale-invariant dependency measures can be expressed in terms of an underlying copula function. 

The theorem can be applied in both directions: First, to model the dependence structure of multivariate distributions independent of their underlying marginal distributions and second, to construct bivariate or multivariate distributions based on a copula function and marginal distributions \parencite[302-304]{Sempi2011}. 

The splitting of a multivariate distribution into its marginals and a copula allows for a two-stage estimation process in multivariate models: The marginal distributions and the copula function can be estimated separately. By doing so, semi-parametric techniques can be utilized \parencite[777]{Patton2009}. For instance, the approach described in the following chapters uses the non-parametric empirical distribution function (EDF) for the marginals while the copula function is estimated parametrically.

Originally, copulas were formulated for static data, where the observations are independent of each other. But recently, copula theory has been adapted to time series: Here, they are used in two ways: First, to describe the cross-sectional dependence structure by estimating the conditional copula function of the conditional joint distribution at some time point given past information. Structural break tests or time series models for the parameters of the copula function can then applied to analyze temporal changes in the dependence structure. Second, copulas are used to describe the dependence between observations of a univariate time series. This is related to the study of Markov processes. \parencite[p. 771]{Patton2009}.

This thesis focuses on dependence modeling for multivariate time series. We try to summarize and structure the current development in the field of factor copula models, which are a specific class of copulas suitable for high dimensional applications. For this type of model, the dependence is modeled as a linear factor structure for which the dependencies can be described by a lower dimensional set of latent variables. Furthermore, this paper contributes to the scientific discussion in two ways:

First, a software library, written in the language \emph{R} is presented. It allows the consistent specification, simulation and estimation of general factor copula models. By doing so, the methods and the structural break test are made available to a broader scientific audience and can be easily studied and improved by other researchers. 

Second, we study a novel way of applying the discussed methods to a non-finance related area of research. Specifically, the case of the German and European refugee crisis in late 2015 and its perception by political parties on the social media platform Facebook is analyzed.

Using an aggregated dataset of textual social-network posts from German politicians and political parties we derive a dynamic party-specific measurement of topic salience. Topic or issue salience is a fuzzy concept used in the political sciences to describe the importance or prominence which various actors in a political system such as voters, parties or elites, place on certain issues \parencite[557]{Wlezien2005}. In this study, the importance of refugee and asylum related issues in the political online discourse is measured separately for each party over time. Using the methods of dependence modeling and the structural break test we ask whether and how the dependencies in topic salience between the parties changed due to the refugee crisis. 

In the following, we give an overview over the next chapters: The first chapter deals with the theoretical basics of copula theory, introduces the time series framework and the factor copula model. The dynamic model is based on the assumption of a time-invariant copula given that each marginal time series is filtered by its time dependent conditional means and variances. Further, the estimation method known as Simulated Method of Moments (SMM) is presented. It is based on the comparison of a vector of dependencies measures calculated independently with simulations from the factor model and with the observed residual data. The last theoretical section deals with a structural break test for the possibly time varying parameters of the copula.

The second chapter gives an overview over the software library and discusses the underlying numerical optimization routines. Its validity and functionality of the theory and its implementation is tested in two simulation studies. 

The third chapter focuses on the empirical application. The social-media dataset, its aggregation and the derivation of the topic salience measurements are described. Then the marginal models for the univariate time series are discussed. Finally, the results of applying various factor copula models and structural break tests to the data are presented. The last chapter discusses the results and gives a critical assessment of the thesis and its methods. 

# Theoretical foundation

In this chapter we present and summarize the theoretical foundation of this thesis. First, the general idea behind copula functions is introduced. Second, a special class of copula models, the so called \emph{factor copula} model is presented. Third, we discuss copula models in the context of time series data and present a specific framework for modeling dependencies of multivariate time series. Fourth, the Simulated methods of moments (SMM) and its origin in the Generalized methods of moments (GMM) is presented. The SMM is the estimation method which is used throughout this paper. Finally, we summarize the ideas of a structural break test for possibly time varying parameters of a factor copula model. 

## Copula theory
\label{s:copulatheory}


The joint cumulative distribution function (cdf) $F_Y(y_1, \dots, y_N) = P(Y_1 \leq y_1, \dots, Y_N \leq y_N)$ for some multivariate random vector $Y$ of dimension $N$ has the continuous marginal distributions $F_{Y_i}(y_i) = P(Y_i \leq y_i) ~\forall~ i = 1, \dots, N$. Estimating $F_Y$ is computationally demanding especially if $N$ becomes large. Therefore, a copula function is introduced which can be used to link the marginal and the joint cdf.

### Definitions

A function of the type $C: [0, 1]^N \rightarrow [0,1]$, with $N \geq 2$ is called a \emph{copula} if it is the distribution function of a random vector $U$ such that first, $C_U(u_1, \dots, u_N) = P(U_1 \leq u_1, \dots, U_N \leq u_N)$ and second, its marginal distributions are $U_i \sim Unif(0, 1)$, e.g. uniformly distributed \parencite[p. 7]{Joe2015}.\footnote{We usually use the letter $F$ to denote a cdf but the letter $C$ is reserved for the copula, e.g. a cdf with uniformly distributed marginals.} The theorem by \textcite{Sklar1959} states, that every d-variate distribution function $F_Y(y_1,\dots, y_n)$ can be expressed in terms of its marginal distributions and a copula function such that 
\begin{equation}
\label{eq:sklar}
F_Y(y_1, \dots, y_N) = C_U(F_1(y_1), \dots, F_N(y_N)).
\end{equation}
To see this, first consider the so called probability-integral transformation. It states that the random variables $U_i = F_{Y_i}(Y_i)$ are uniformly distributed \parencite[4]{Embrechts2002}. This is due to the fact that $F_{U_i}(u_i) = P(U_i \leq u_i) = P(F_{Y_i}(Y_i) \leq u_i) = P(Y_i \leq F_{Y_i}^{-1}(u_i)) = F_{Y_i}(F_{Y_i}^{-1}(u_i)) = u_i$.  $F_{U_i}(u_i) = u_i$ is exactly the definition of the distribution function of a $Unif(0,1)$ distributed variable. Using this transformation we can write
\begin{align}
\label{eq:proofsklar}
\begin{split}
F_Y(y_1, \dots, y_N) &= P(Y_1 \leq y_1, \dots, Y_N \leq y_N)\\
&= P(F_{Y_1}(Y_1) \leq F_{Y_1}(y_1), \dots, F_{Y_N}(Y_N) \leq F_{Y_N}(y_N)) \\
&= P(U_1 \leq F_{Y_1}(y_1), \dots, U_N \leq F_{Y_N}(y_N)) \\
&= C_U(F_{Y_1}(y_1), \dots, F_{Y_N}(y_N)).
\end{split}
\end{align}
Note that the transformation of the marginal distributions to a $Unif(0, 1)$ distribution is somehow an arbitrary choice and other possibilities, for instance a transformation to a Gaussian distribution, are also used in other areas of research \parencite[7]{Mikosch2006}.

If $F_Y$ is a continuous function with marginal quantile functions $F^{-1}_{Y_1}, \dots, F^{-1}_{Y_N}$ then the copula function $C_U(\mathbf{u})$ is uniquely determined by $C(\mathbf{u}) = F(F_{Y_1}^{-1}(u_1), \dots, F_{Y_N}^{-1}(u_N))$. For other non-continuous functions this must not be the case and several valid copula functions can exist \parencite[4-5]{Embrechts2002}.

The simplest form of a copula is the copula of a vector of independent variables for which the cdf can be written as $F_Y(\bm{y}) =  \prod_{i = 1}^N F_{Y_i}(y_i)$. Using the derivation in \eqref{eq:proofsklar}, the corresponding independence copula is given by
\begin{equation}
C_U^{ind}(u_1, \dots, u_N) = \prod_{i = 1}^N u_i.
\end{equation}

Many other possible functional forms for a copula $C_U$ exist in the literature (see \textcite{Joe2015} for an overview). In this paper, we focus on a special class, the so called factor copulas which we present in section \ref{s:factorcopula}. 

### Copulas and dependency

Copulas are useful for the study of dependence between a set of random variables because the copula function is unaffected by monotonic transformations of the marginal variables. In contrast to the linear correlation coefficient, this property allows the definition of alternative scale-invariant dependence measures which only dependent on the copula function \parencite[125]{Nelsen1999}. 

To see the scale-invariant property of the copula function, consider a random vector $U$ with distribution function $C_U(\bm{u})$ and $C_U(F_1(y_1), \dots, F_N(y_N)) = F_Y(y_1, \dots, y_N)$. By applying some increasing functions $T_i(Y_i)$ to the random variables and using \eqref{eq:proofsklar} we can write
\begin{equation}
\begin{split}
F_{T(Y)}(\bm{y}) &= P(T_1(Y_1) \leq y_1, \dots, T_N(Y_N) \leq y_N)\\
&=  P(T_1(F_{Y_1}^{-1}(U_1)) \leq y_1, \dots, T_N(F_{Y_N}^{-1}(U_N)))\\
&= C_U(F_{T_1(Y_1)}(y_1), \dots, F_{T_N(Y_N)}(y_N)).
\end{split}
\end{equation}
Thus, although $Y$ and $T(Y)$ have different joint distribution functions, they share the same copula function \parencite[6]{Embrechts2002}. 

In the following we shortly present three common scale invariant measures of dependency between two variables $Y_i$ and $Y_j$. In contrast to the linear correlation coefficient, these measures can be expressed solely as a function of the underlying bivariate copula.\footnote{For a proof of the copula representations see \textcite[16-18]{Embrechts2002}.} Spearman's and Kendall's rank correlation both measure the degree of monotonic dependence between the two variables $Y_i$ and $Y_j$ with joint distribution function $F_Y(\bm{y})$. As the linear correlation coefficient they are symmetric and normalized in the interval $[0, 1]$ \parencite[15]{Embrechts2002}.

Kendall's rank correlation is based on the definition of concordance and discordance: Consider two pairs of independent and identically distributed (iid) random vectors $(Y_i^1, Y_j^1)$ and $(Y_i^2, Y_j^2)$. Both pairs share the same joint distribution $F_Y$. The pair is concordant if $(Y_i^1 - Y_i^2)(Y_j^1-Y_j^2) > 0$ and discordant if $(Y_i^1-Y_i^2)(Y_j^1-Y_j^2) < 0$. For the former case large (small) values of one pair occur with large (small) values of the other. For the latter case, large (small) values of one pair occur with small (large) values of the other \parencite[125-126]{Nelsen1999}.

With this definition Kendall's rank correlation is given as the probability of concordance minus the probability of discordance:
\begin{align}
\begin{split}
\label{eq:kendalls}
\tau_{i,j} &= P((Y_i^1-Y_i^2)(Y_j^1-Y_j^2) < 0) - P((Y_i^1-Y_i^2)(Y_j^1-Y_j^2) > 0)\\
&= 2 \cdot P((Y_i^1-Y_i^2)(Y_j^1-Y_j^2) > 0) -1 \\
&= 4\int\int_{[0,1]^2} C(u_i, u_j)dC(u_i, u_j)-1.
\end{split}
\end{align}

Spearman's rank correlation is given as the linear correlation coefficient between the probability-integral transforms $F_{Y_i}(Y_i)$ and $F_{Y_j}(Y_j)$:
\begin{align}
\begin{split}
\label{eq:spearman}
\rho_{i,j}^S &= \rho(F_{Y_i}(Y_i), F_{Y_j}(Y_j)) = 12E(F_{Y_i}(Y_i), F_{Y_j}(Y_j))-3\\
&= 12\int\int_{[0,1]^2} u_iu_j dC(u_i, u_j)-3.
\end{split}
\end{align}

Finally, to capture dependencies in the joint lower or joint upper parts of the distribution, one defines the coefficients of upper and lower tail dependency: 
\begin{align}
\label{eq:taildependency}
\begin{split}
\lambda^U_{i, j} &= \underset{q \rightarrow 1}{\lim} P(Y_i > F_{Y_i}^{-1}(q) | Y_j > F_{Y_j}^{-1}(q)) 
= \underset{q \rightarrow 1}{\lim} \frac{1-2q+C(q,q)}{1-q},\\
\lambda^L_{i, j} &= \underset{q \rightarrow 0}{\lim} P(Y_i \leq F_{Y_i}^{-1}(q) | Y_j \leq F_{Y_j}^{-1}(q))
= \underset{q \rightarrow 0}{\lim} \frac{C(q,q)}{q}.
\end{split}
\end{align}
The coefficients measure the probability that extreme large (small) values occur in one variable, given extreme large (small) values in the other variable. In contrast to other common dependence measures, the coefficients of upper and lower tail dependency are defined on the interval $[0, 1]$. 

Since the limit only exists theoretically and not for observable data, one usually calculates upper and lower \emph{quantile} dependency for some values of $q$ close to $0$ for lower and close to $1$ for upper quantile dependence \parencite[62-63]{Joe2015}. The sample versions of these measures can be obtained by replacing the marginal distributions with the EDF and using the definition of the empirical copula \parencite[176]{Nelsen1999}:
\begin{equation}
\hat{C}_{i,j}(u, v) = \frac{1}{T+1}\sum_{t=1}^{T}\bm{1}(\hat{F}_i(x_{it}) \leq u, \hat{F}_j(x_{jt})\leq v).
\end{equation}
It counts the relative number of pairs in a sample $\{(x_{it}, x_{jt})\}_{t=1}^{T}$ whose rank is less than $u \cdot T$ and $v \cdot T$ respectively. 

## Factor copulas
\label{s:factorcopula}

Factor copulas are a special class of copula models for which the copula function $C_U(u_1, \dots, u_N)$ is based on a latent factor structure as defined in \textcite{Patton2013, Patton2017}. \footnote{Note, that \textcite[p. 128]{Joe2015} developed another type of copula model under the same term \emph{factor copula}. There, the dependence structure can be defined through conditional distributions for which the conditioning set is based on some latent variables.}

### General set up

Consider a set of artificial variables $X_i, i = 1, \dots,N$ which linearly depend on some latent factors $Z_k, k = 1, \dots, K, K < N$ and some iid error $e_i$ such that $X_i =  \sum_{k = 1}^{K}\beta_{ik}Z_k+ \epsilon_i$. The linear coefficients $\beta_{ik}$ are also called factor loadings. The latent variables $Z_k$ and the error term $\epsilon_i$ follow some parametric distributions with parameter vectors $\bm{\gamma}_\epsilon$ and $\bm{\gamma}_{Z_k}$ and one can write: $\epsilon_i \overset{iid}{\sim} F_\epsilon(\bm{\gamma_\epsilon})$ and $Z_k \sim F_{Z_k}(\bm{\gamma_{Z_k}})$. While the variables $X_i$ usually dependent on each other, the latent factors are independent of each other and from the error term. 

As shown in the previous section, the joint probability function $F_{X}(x_1, \dots, x_N)$ of the artificial variables can be expressed in terms of its marginal distributions $F_{X_i}(x)$ and a factor copula function $C_U(u_1, \dots, u_N; \btheta)$ such that $F_{X}(x_1, \dots, x_N) = C_U(F_{X_1}(x_1), \dots, F_{X_N}(x_N); \btheta)$.

The artificial variables $X_i$ are only used to construct the factor copula function $C_U(u_1, \dots, u_N; \btheta)$. The parameters of the factor structure are chosen in such a way that the resulting copula function fits the copula of the observed random vector $Y$, such that $F_{Y}(y_1, \dots, y_n) = C_U(F_{Y_1}(y_1),\dots, F_{Y_N}(y_N); \btheta)$. Once the factor copula function is determined, the artificial variables and its marginal distributions $F_{X_i}(x)$ are of no interest.

The parameters of the factor model are collected in a parameter vector $\btheta = (\beta_{11}, \dots, \beta_{i1}, \dots \beta_{ik}, \bm{\gamma_{Z_1}}', \dots, \bm{\gamma}_{Z_K}', \bm{\gamma}_\epsilon')'$. It consists of all linear coefficients and the distributional parameters of the error term and the latent variables. The number of latent variables $K$ and the distribution functions $F_{Z_1}, \dots, F_{Z_k}, F_\epsilon$ are hyper-parameters of the model which have to be chosen prior to the estimation.\footnote{\textcite[143]{Patton2017} provide a heuristic of finding the number of latent variables by analyzing so called \emph{scree-plots}: Ordered eigenvalues from the sample rank-correlation matrix of the data.}

Using matrix notation, the model can be summarized in the following set of equations:
\begin{align}
\label{eq:factorcopula}
\begin{split}
Y &= (Y_1, \dots, Y_N)' \\
X &= (X_1, \dots, X_N)' = \bm{\beta} \cdot Z + \bm{\epsilon}\\
F_{Y}(\bm{y}) &= C_U(F_{Y_1}(y_1),\dots, F_{Y_N}(y_N); \btheta)\\
F_{X}(\bm{x}) &= C_U(F_{X_1}(x_1), \dots, F_{X_N}(x_N); \btheta)
\end{split}
\end{align}

To model the joint cdf $F_Y(\bm{y})$, a two-stage estimation process can be used: First, the marginal distributions $\hat{F}_{Y_i}$ are estimated parametrically or non-parametrically, for instance by using some parametric model or the EDF. Second, the factor structure for the copula function is fitted to the data by finding the optimal $\hat{\btheta}$. In most cases, a closed form of the factor copula does not exist such that one cannot directly relate the copula parameters to the dependency measures as given by \eqref{eq:kendalls} - \eqref{eq:taildependency}. Therefore, one can adapt simulation based estimation methods as described in section \ref{s:SMM}. 

### Model restrictions
\label{s:restrictions}

An upper bound for the number of model parameters $P = |\btheta|$ to be estimated is given by the size of the factor matrix and the number of additional free distributional parameters such that $P \leq (N \cdot K + |\bm{\gamma_{Z_1}}| + \dots + |\bm{\gamma_{Z_K}}| + |\bm{\gamma_\epsilon}|)$. To reduce the number of parameters, \textcite[148, 150]{Patton2017} present two restrictions on the matrix of factor loadings $\bm{\beta}$: the restrictive \emph{equidependence} and the less restrictive \emph{block-equidependence} model. 

For the first model, it is assumed that $K = 1$ and $\bm{\beta} = (\beta, \dots, \beta)'$. Thus, the model consists of a single latent factor and a single factor loading $\beta$ which is the same for all variables. This set up implies equal pairwise dependencies for all observable variables.  

\begin{figure}[H]
	\includegraphics[width=\textwidth]{./figures/fig1}
	\caption[Simulations from different equidependence factor copula models]{Simulations from different equidependence factor copula models with $N = 2$, $\beta = 1.5$, $Y_i \sim N(0, 1)$ and different distributions for the latent variable and the error term.}	
	\label{f:equidependence}  
\end{figure}

Figure \ref{f:equidependence} shows four different simulations from a two-dimensional one factor equidependence factor copula model. The marginal distributions are standard normal, the linear coefficient is fixed at $\beta = 1.5$ but the distributions of the latent variable and the error term differ. All models produce positive dependence but the symmetry and tail dependency differs through the choice of the distributions and their parameters. The upper left panel shows realizations from a factor copula model with a skew-t distributed latent factor and a t-distributed error term. 

The skew-t distribution is a generalization of the t-distribution and allows the specification of asymmetry via a parameter $\lambda \in (-1, 1)$. Note, that the normal and t-distribution can be seen as special cases of the skew-t distribution. For $\lambda = 0$ the t-distribution and for $df \rightarrow \infty$ the normal distribution is obtained \parencite[709, 710]{Hansen1994}.

In this example, the degrees of freedom are set to $df = 4$ and the skewness parameter to $\lambda = -0.8$. This produces strong asymmetric tail dependencies. The factor copula in the bottom left panel has similar distributions. But here symmetric tail dependencies are produced since $\lambda = 0$. This results in an ordinary t-distribution for the latent variable. The bottom right panel shows realizations from a factor model for which both the error term and the latent variable are normal distributed. This yields a multivariate normal distribution for the artificial variables $X$ and the corresponding copula is the Gaussian copula with no tail dependency or asymmetry. The last panel shows the combination of an exponential latent variable with a normal distributed error term. This produces strong upper tail dependencies. 

The block-equidependence model, is less restrictive than the equidependence model. It is suitable for higher dimensional problems and for variables which can be naturally partitioned into different groups.\footnote{For instance this could be stock market prices grouped into different industry sectors.} The model assumes a common latent factor for all groups and a group specific factor for each group. Thus, each artificial variable $X_i$ is only affected by two factors. For the matrix of factor loadings, it is further assumed that all variables in the same group have the same factor loading while variables in different groups can have different loadings. This structure implies equal pairwise intra-group dependencies while the pairwise inter-group dependencies can vary between the groups. 

Formally, consider a partition of $\bm{Y} = (Y_1, \dots, Y_N)'$ into $M$ groups. A single variable can then be written as $Y_i^j$, where $i = 1, \dots, N$, $j = 1, \dots, M$. The value $k_j$ is the number of variables in group $j$ and it holds that $\sum_{j = 1}^M k_j = N$. Then the factor copula model can be summarized as:

\begin{align}
\label{eq:block-equidependence}
\begin{split}
\bm{X} &= (X^1_1, \dots, X_{k_1}^1, X_{k_1+1}^2, \dots, X_{k_1+k_2}^2, \dots, \dots, X_N^{M})' = \bm{\beta Z} + \bm{\epsilon}\\
\bm{Z} &= (Z_0, Z_1, \dots, Z_M)'\\
X_i^j &= \beta_j Z_0 + \beta_{M+j} Z_j + \epsilon_i.
\end{split}
\end{align}
The loadings matrix $\bm{\beta}$ is of size $N \times (M+1)$ but with only $2M$ actual factor loadings. It has the following structure:
\begin{aling}
\begin{split}
\bm{\beta} &= \begin{pmatrix}
\beta^1	& \beta^{M+1} & 0			&\cdots & 0 \\
\beta^1 & \beta^{M+1} & 0			&\cdots & 0 \\
\vdots 	& \vdots 	  & \vdots 		&\ddots & \vdots \\
\beta^1 & \beta^{M+1} & 0			&\cdots & 0 \\
\beta^2 & 0  		  & \beta^{M+2}	&\cdots & 0  \\
\vdots 	& \vdots 	  & \vdots		&\ddots & \vdots \\
\beta^M & 0			  & 0			&\cdots & \beta^{M+M}\\
\vdots 	& \vdots 	  & \vdots		&\ddots & \vdots \\
\beta^M & 0			  & 0			&\cdots & \beta^{M+M}\\
\end{pmatrix}.
\end{split}
\end{align}

## Copula models for multivariate time series
\label{s:dynamiccopula}

Up to now, it was implicitly assumed, that the observations sampled from the random variables $Y$ are independent of each other. But one can extent the copula model to univariate or multivariate time series processes for which each observation depends on past realizations. For the former case, the copula is used to estimate the joint distribution of a one dimensional time series $(Y_t, Y_{t+1}, \dots, Y_{t+n})'$. For the latter, the interest lies in the conditional joint distribution of the time dependent random vector $Y_t = (Y_{1t}, \dots, Y_{Nt})'$. The conditional cdf can be written as $F_{Y_t|\mathcal{F}_{t-1}}(\bm{y})$,  where the $\sigma$-algebra $\mathcal{F}_{t-1}$ contains information from past realizations of $Y$ and other exogenous variables \parencite[4-6]{Patton2012}. 

Sklar's theorem given by \eqref{eq:sklar} shows the connection between a copula function, a multivariate distributions and its marginal cdfs. It can be extended to the multivariate time series case for which the conditional distribution is split into a conditional copula $C_{U_t|\mathcal{F}_{t-1}}(\bm{u})$ and conditional marginal distributions $F_{Y_{it}|\mathcal{F}_{t-1}}(y)$:
\begin{equation}
\label{eq:conditionalcopula}
F_{Y_t|\mathcal{F}_{t-1}}(y_1, \dots, y_N) = C_{U_t|\mathcal{F}_{t-1}}(F_{Y_{1t}|\mathcal{F}_{t-1}}(y_1), \dots, F_{Y_{Nt}|\mathcal{F}_{t-1}}(y_N)).
\end{equation}
To have a valid conditional multivariate distribution, the conditioning set must be the same for the marginal distributions and the copula \parencite[p. 772]{Patton2009}. 

For the following sections, another approach is chosen. We built on a semi-parametric copula-based multivariate dynamic model as described in \textcite[126-129]{Chen2006}. First, the vectors with the conditional means and variances of $Y_t|\mathcal{F}_{t-1}$ are estimated parametrically. They are used to control for possible time dependencies. Second, the standardized residuals are calculated and their joint distribution is then modeled using non-parametric estimates of the marginal distributions and a parametric copula.

Hence, this model describes three levels of estimation: First, the dynamic parameters of the marginal models for the conditional time varying means and variances. Second, the estimation of the marginal distribution of the standardized residuals. Here, this is done non-parametrically with the EDF. Third, the parameters of the copula function for the standardized residuals. 
  
If we denote the parametric conditional mean of a single variable as $\mu_{it} = E(Y_{it}|\mathcal{F}_{t-1}; \bm{\phi})$ and the parametric conditional standard deviation as $\sigma_{it} = \sqrt{V(Y_{it}|\mathcal{F}_{t-1}; \bm{\phi})}$ we can write the multivariate time series process as
\begin{equation}
\label{eq:model}
Y_t = \bm{\mu}_t + \bm{\sigma}_t\eta_t,
\end{equation}
with $\bm{\sigma}_t = \diag(\sigma_{1t}, \dots, \sigma_{Nt})$ and $\bm{\phi}$ the parameter vectors of the univariate time series models. 

The standardized random vector of residuals $\eta_t = (\eta_{1t}, \dots, \eta_{Nt})' = \bm{\sigma}_t^{-1}(Y_t - \bm{\mu}_t)$ is independent of past realizations since it is assumed that all dynamics are captured by the conditional mean and standard deviation. Using a parametric copula, the joint cdf of the residuals can be written as:
\begin{equation}
F_{\eta}(u_1, \dots, u_t) =  C_U(F_{\eta_1}(u_1), \dots, F_{\eta_N}(u_N); \btheta).
\end{equation}

The first level parameters $\bm{\phi}$ of the conditional means and variances are estimated with standard parametric models for univariate time series, such as models from the GARCH family \parencite[17]{Teraesvirta2009}. Given a sample of observed residuals $\{\bm{\eta}_t\}_{t = 1}^T$, their marginal distributions are estimated with the EDF:
\begin{equation}
\hat{F}_{\eta_i}(u) = \frac{1}{T+1}\sum_{t = 1}^{T}\bm{1}(\eta_{it} \leq u).
\end{equation}
The copula which describes the dependence structure between the residuals, can be modelled with a factor copula function and estimated with the SMM technique which is presented in the following chapter. 

## Simulated Method of Moments estimation for factor copulas
\label{s:SMM}

Based on the time series model as shown in the previous section \ref{s:dynamiccopula} and the factor copula model as defined in section \ref{s:factorcopula}, an estimation procedure known as Simulated Method of Moments is presented. 

In models for which the copula function is known in closed form, standard Maximum Likelihood (ML) estimation can be performed. Given the time series model as described by \eqref{eq:model} multistage ML can be applied for estimating the first level conditional means and variances, the marginal distribution of the residuals and the copula. \textcite[127]{Chen2006} showed that, if the EDF is used for the estimation of the marginal distribution of the residuals, the first level estimates do not affect the asymptotic distribution of the semi-parametric ML estimator of the copula parameters. 

Since the factor copula implied by \eqref{eq:factorcopula} is not given in closed form approximating the Likelihood function is difficult and computationally demanding \parencite[694]{Patton2013}. On the other hand, it is easy and computationally cheap to simulate many values from the factor model. Therefore, an alternative approach is used. It is based on the Generalized Methods of Moments (GMM), for which its moment conditions are calculated with simulated values. As shown by \textcite{Chen2006} for the semi-parametric ML estimator, the conditional distribution of the SMM estimator is also not affected by the first level dynamic estimates of the time series model in \eqref{eq:model} \parencite[692]{Patton2013}. 

### SMM and the Generalized Method of Moments

SMM and GMM estimation is rooted in the Method of Moments. For many statistical parametric models, theoretical population moment conditions as functions of the model parameters and the random variables can be derived. For estimation, the expectation in these conditions can then be replaced by the respective sample average \parencite[6-7]{Hall2005}. 

Formally, the population moment conditions take the form $g(\btheta) = E({m}(\eta_t)) - m_0(\btheta)$ with $g(\btheta_0) = 0$. We define $m(\eta_t)$ to be a vector of moment functions applied to the random vector of residuals, $\btheta_0$ the vector of true parameters and $m_0(\btheta)$ the theoretical moments as functions of the model parameters. Given the true vector of parameters the moments match their theoretical counterparts. For instance for a normal variable $X \sim N(\mu, \sigma)$ the condition can simply be written as $E(X) - \mu = 0$. 

The moment conditions can be estimated by replacing the expectation with the sample average. If exactly as many moment conditions as model parameters exist, the system can be solved exactly. If more moment conditions than parameter are available no unique solution exist and one minimizes the distance of the sample estimator $g_T(\bm{\eta})$ to 0. Given a sample of observed residuals $\{\bm{\eta}_t\}_{t = 1}^T$ the distance is measured by the weighted quadratic form:
\begin{align}
\begin{split}
\label{eq:GMM}
\hat{\btheta}_{GMM} &= \underset{\btheta}{ \arg \min ~} Q(\btheta) = g_T(\btheta)'W_Tg_T(\btheta)\\
&=  \underset{\btheta}{ \arg \min ~} (\frac{1}{T}\sum_{t = 1}^{T} m(\bm{\eta}_t) - m_0(\btheta))'\hat{W}(\frac{1}{T}\sum_{t = 1}^{T}m(\bm{\eta}_t) - m_0(\btheta)).
\end{split}
\end{align}

Under suitable assumptions and by applying a law of large numbers and the central limit theorem it can be shown that the estimator converges in probability to the true parameters and that it is asymptotically normal distributed. 

The weighting matrix is a positive semi-definite symmetric matrix which must converge in probability to a positive definite matrix of constants. By doing so it is ensured, that a positive weight is applied to all moment conditions. The variance of the GMM estimator depends on the choice of the weighting matrix. Therefore, $W_T$ should be chosen in such a way, that it minimizes the asymptotic variance of the GMM estimator \parencite[14, 43]{Hall2005}.

For the GMM, stating the vector of true moments $m_0(\btheta)$ is often straightforward. In cases, where there exist no explicit mapping from the model parameters to the true moments but the DGP is known and simulations are easy to obtain one can use the SMM \parencite[342-347]{Hall2005}. For this, the theoretical moments $m_0(\btheta)$ are replaced by moments calculated with simulated data $\{\bm{X}_s\}_{s=1}^S$ from the model. The simulated moments are then compared to the observable sample moments. The SMM estimator is therefore similar to the GMM version but the finite sample moment condition changes to
\begin{equation}
\label{eq:SMM}
g_T(\btheta) = \frac{1}{T}\sum_{t = 1}^{T}(m(\bm{\eta}_t)) - \frac{1}{S}\sum_{s = 1}^{S}(m(\bm{X}_s;\btheta)).
\end{equation}

For most cases the SMM estimator is consistent and asymptotically normal distributed. However, since the estimation error from the simulations cannot be neglected, the variance of the SMM is usually larger. The inefficiency vanishes if $\frac{T}{S} \rightarrow \infty$ \parencite[344-345]{Hall2005}. 

### SMM for factor copulas
\label{s:smmfactorcopulas}

The presented estimation method and its asymptotic results are valid for well-defined conditions based on the moments of the variables, e.g. the mean or the variance. For factor copulas, these moments cannot be formulated. The scale-invariant measures of dependency as defined in \eqref{eq:kendalls} - \eqref{eq:taildependency} are functions of the copula. In the estimation approach presented by \textcite{Patton2013}, these dependency measures are used to replacing the moment conditions of the ordinary GMM approach. Therefore, the assumptions and the asymptotic results of the GMM estimator are not applicable. However, as the authors proved, asymptotic results for the distribution of the copula parameters $\hat{\btheta}$ can still be derived. 

For factor copulas, the "moment" conditions for the SMM estimation consist of vector functions of pairwise dependency measures separately calculated with the observed matrix of residuals $\bm{\eta}$ and $S$ simulations $\bm{X}_s$ from the copula model as given by \eqref{eq:factorcopula}. The moment condition as stated in \eqref{eq:GMM} and the SMM adaption \eqref{eq:SMM} is replaced by
\begin{equation}
\label{eq:copulaSMM}
g_T(\btheta) = m_T(\bm{\eta}) - m_S(\bm{X}; \btheta).
\end{equation}
The vector function $m(\cdot)$ generates a vector of pairwise dependency statistics. As measure of dependency, the empirical counterparts of Spearman's Rho as given in \ref{eq:spearman} and lower and upper quantile dependency as given in \ref{eq:taildependency} for $q \in (0.05, 0.1, 0.9, 0.95)$ are chosen. Other invariant measures such as Kendall's Tau can also be included. 

Formally, we can write $m(\cdot) = \{\bm{\delta}_{i, j}\}_{i = 1, j = i}^{N-1, N}$, with $\bm{\delta}_{i, j} = (\hat{\rho}^S_{i, j}, \hat{\lambda}_{i, j}^{0.05}, \hat{\lambda}_{i, j}^{0.1}, \hat{\lambda}_{i, j}^{0.90}, \hat{\lambda}_{i, j}^{0.95})'$. For an unrestricted factor model there exist $5\cdot(0.5 \cdot N \cdot (N-1))$ dependency statistics - 5 for each unique pair of variables. For the simpler equidependence model, which assumes the same factor loading for all variables, one can average over all pairwise combinations such that we can write:

\begin{equation}
\label{eq:equidependence}
m(\cdot) = \frac{2}{N\cdot(N-1)}\sum_{i = 1}^{N-1}\sum_{j = i+1}^{N} \bm{\delta}_{i, j}.
\end{equation}
This results in a single vector of dependencies. 

For the bloc-equidependence model the final number of dependencies is $5\cdot M$ because the model structure allows one to average over all intra- and inter-group dependencies. Using the same notation as in \eqref{eq:block-equidependence}, we can write for each group $m = 1, \dots, M$
\begin{equation}
\bar{\bm{\delta}}_m =  \frac{1}{M}(\underbrace{\sum_{r = 1, r \neq m}^{M} \frac{1}{k_mk_r}\sum_{i = 1}^{k_m}\sum_{j = 1}^{k_r}  \bm{\delta}_{im, jr}}_{\text{inter-group dependencies}} + \underbrace{\frac{2}{k_m(k_m-1)}\sum_{i = k_m}^{k_m-1}\sum_{j = i+1}^{k_m} \bm{\delta}_{im, jm}}_{\text{intra-group dependencies}}),
\end{equation}
where $\bm{\delta}_{im, jr}$ is the vector of dependencies for the $i$th variable in group $s$ and the $j$th variable in group $r$. Finally, for the bloc-equidependence model, the moment function can be written as: $m(\cdot) = (\bar{\bm{\delta}}_1, \dots, \bar{\bm{\delta}}_M)'$. 

\textcite[p. 691]{Patton2013} showed that under some regularity assumptions the SMM estimator is weakly consistent and asymptotically normal distributed. The assumptions ensure that for both the iid and the time series case, the sample dependency measures converge in probability to their theoretical population values. Notably, it is assumed that the population version of the moment condition $g(\btheta)$ is differentiable at the true parameter $\btheta_0$ while this is not true for the sample version $g_T(\btheta)$.  

The convergence of the estimator can be summarized as:
\begin{align}
\label{eq:asymptotic}
\begin{split}
\hat{\btheta}_{SMM} \sim N(\btheta_0, (\frac{1}{T}+\frac{1}{S})\bm{\Omega}) \text{ ~for~ }T, S \rightarrow \infty,
\end{split}
\end{align}
with covariance matrix $\bm{\Omega} = (G'WG)^{-1}G'W'\Sigma WG(G'WG)^{-1}$, where $G = \frac{\partial g(\btheta)}{\partial \btheta_0}$ is the Jacobian matrix of the first order derivatives of the moment condition $g(\btheta)$ and $\Sigma$ is the asymptotic variance of the vector of dependencies $m_T(\bm{\eta})$ calculated with the observed sample of residuals.

The weight matrix $W$ can be chosen to be the identity matrix which results in some efficiency losses. On the other hand, the efficient weight matrix which minimizes the variance of the estimator is given by $W = \Sigma^{-1}$ and the overall variance reduces to $\Omega = (G'WG)^{-1}$. In contrast to ordinary GMM estimation, the efficient weight matrix does not depend on the parameters of the factor copula model and can be estimated consistently before estimating the parameters $\btheta$. 

The asymptotic variance $\Sigma$ of the sample moments $m_T(\bm{\eta})$ can be estimated with an iid bootstrap procedure:

\begin{algorithm}
\BlankLine
\KwIn{$\{\bm{\eta}_t\}_{t = 1}^T$}
\KwOut{$\Sigma_{T,B}$}
$\hat{m}_T \gets m_T(\bm{\eta})$\;
  \For{$b\gets1$ \KwTo $B$}{
    $\bm{\eta}_t^{(b)} \gets$ sample $T$ values with replacement from $\{\hat{\bm{\eta}}_t\}_{t = 1}^T$\;
$\hat{m}_T^{(b)} \gets m_T(\bm{\eta}^{(b)})$\;
    }
    \BlankLine
    $\Sigma_{T,B} \gets \frac{T}{B}\sum_{b = 1}^B(\hat{m}_T^{(b)} - \hat{m}_T)(\hat{m}_T^{(b)} - \hat{m}_T)'$\;
    \BlankLine
    \KwRet $\Sigma_{T,B}$\;
  \caption{Bootstrap procedure for estimating the asymptotic variance $\Sigma_{T,B}$}
\label{alg:sigma}
\end{algorithm}

Here, $B$ samples with replacement are taken from the residuals and the scaled sample covariance is calculated. Note that in the algorithm the mean over all sample moments $\hat{m}_T^{(b)$ was replaced by the actual moments $\hat{m}_T$ since it holds that $E(\frac{1}{B}\sum_{b = 1}^{B}\hat{m}_T^{(b)}) = \hat{m}_T$.  

The derivative $G$ can be estimated using a numerical approximation around the parameter estimator $\hat{\btheta}$. For the $k$th column of $G_{T, S}$ one can write:
$$
G_{T, S, k} = \frac{g_{T, S}(\hat{\btheta}+\bm{e}_k\epsilon_{T, S}) - g_{T, S}(\hat{\btheta}-\bm{e}_k\epsilon_{T, S})}{2\epsilon_{T, S}},
$$
where $\bm{e}$ is the $k$th unit vector and $\epsilon_{T, S}$ the step size which is usually set to $\epsilon_{T, S} = 0.1$. 



## A structural break test for factor copulas
\label{s:breaktest}

The dynamic model as given by \eqref{eq:model} allows for a wide variety of parameterization and copula functions. Here, we focus on the factor copula model and the SMM estimation procedure as presented in the previous sections. In the dynamic model, the dependence structure of the residuals is assumed to be static since all dynamics are captured by the conditional mean and variance of the marginal distributions. But given the findings of time varying correlations in real datasets the assumption of a static copula seems unrealistic \parencite[p. 655]{Manner2012}. 

Different approaches exist to allow for a time varying dependence structure \parencite[658 - 668]{Manner2012}: For instance, the parameters of the copula can be modeled as functions of the lagged variables, auto-regressive terms or as an independent stochastic process. But one can also test for a structural break in the copula parameters at some point in time $t$. These approaches assume that the functional form of the copula doesn't change. Other methods explicitly allow multiple alternating copula functions. 

In the following section a structural break test based on \textcite{Wied2017} for a change of the parameters of the factor copula as given by \eqref{eq:model} is presented. Note that it is assumed that the underlying factor copula function doesn't change, while the copula's parameters are allowed to vary over time. 

The test is based on recursive estimations of the copula model at each time point $t \in \{\lfloor \epsilon T \rfloor, \dots, T\}$, with $0 < \epsilon < 1$. We write the recursive parameter as $\btheta_t$. For its estimation, all information from the first period up to point $t$ is used. The strictly positive trimming parameter $\epsilon$ has to be chosen such that the model parameters don't fluctuate too much due to the larger estimation error of smaller sample sizes. In the following applications we usually choose $\epsilon$ such that the first recursive period starts at $t = 300$. 

The null hypothesis states that the parameters of the factor copula model are stationary while the alternative assumes at least one significant break point in time:
\begin{equation}
H_0: \btheta_1 = \btheta_2 = \dots = \btheta_T \hspace{2em} H_1: \btheta_t \neq \btheta_{t+1} \text{ for some } t = \{1, \dots, T\}.
\end{equation}

For each period, the recursive estimator $\hat{\btheta}_t$ is compared to the estimated parameters of the full model $\hat{\btheta}_T$. If the recursive estimator is significantly far away from the full model estimates, $H_0$ is rejected. Formally, the recursive statistics are calculated as the scaled squared distances between the full model and the recursive estimator. The final test statistic is then given as the maximum distance:
\begin{equation}
P = \underset{\epsilon T \leq t \leq T}{\max} (\frac{t}{T})^2T(\hat{\btheta}_t- \hat{\btheta}_T)'(\hat{\btheta}_t - \hat{\btheta}_T).
\end{equation}

\textcite[10]{Wied2017} also propose a non-parametric alternative test statistic which is solely based on the moment functions calculated with the residuals up to point $t$ as described in section \ref{s:smmfactorcopulas}. It does not involve the recursive estimation of a copula model:
\begin{equation}
M = \underset{\epsilon T \leq t \leq T}{\max} (\frac{t}{T})^2T(m_t(\bm{\eta}) - m_T(\bm{\eta}))'(m_t(\bm{\eta}) - m_T(\bm{\eta})),
\end{equation}
where $m_t(\bm{\eta})$ is the recursive vector of dependencies calculated with the residuals up to time $t$ (compare to \eqref{eq:copulaSMM}).  

For the break point detection, the test statistic is compared to a critical value (CV). The CV is based on the $1-\alpha$ quantile of the distribution of $M$ or $P$ given that $H_0$ is true. It can be simulated with a bootstrap procedure as described in algorithm \eqref{alg:breaktest}. If the test statistic is larger than the CV, $H_0$ is rejected and the corresponding time point of the significant test statistic is chosen as the breakpoint. 

\textcite[13]{Wied2017} show that the test statistics converges in distribution to 
\begin{align}
\begin{split}
P &\overset{d}{\rightarrow} \underset{\epsilon T \leq t \leq T}{\max} (\bm{A}^*(\frac{t}{T}) - \frac{t}{T}\bm{A}^*(1))'(\bm{A}^*(\frac{t}{T}) - \frac{t}{T}\bm{A}^*(1)),\\
M &\overset{d}{\rightarrow} \underset{\epsilon T \leq t \leq T}{\max} (\bm{A}(\frac{t}{T}) - \frac{t}{T}\bm{A}(1))'(\bm{A}(\frac{t}{T}) - \frac{t}{T}\bm{A}(1)),
\end{split}
\end{align}
with $\bm{A}^*(\frac{t}{T}) = ((G'WG)^{-1}G'W(A(\frac{t}{T}) - \frac{t}{T}\sqrt{(\frac{T}{S})}A(1))$ and $T, S \rightarrow \infty$. As for the SMM estimator, $G$ is the matrix of first order derivatives of the population moment condition $g(\btheta)$, $W$ is the weighting matrix of the SMM estimator and $A(\cdot)$ is some Gaussian process as defined in \textcite[39]{Wied2017}. The necessary regulatory assumptions to derive this result are similar in spirit to the assumptions used to derive the asymptotic distribution of the parameters of the factor copula model in section \ref{s:smmfactorcopulas}.

Given a sample of residuals, the distribution of the test statistics under the null hypothesis can be estimated using the following bootstrap procedure:
\begin{algorithm}
\KwIn{$\{\bm{\eta}_t\}_{t = 1}^T$}
\KwOut{$\{\bm{K}_b\}_{b = 1}^B$}
$\hat{m}_T \gets m_T(\bm{\eta})$\;
  \eIf{test based on $P$}{
      $L \gets (G_T'W_TG_T)^{-1}G_T'$\;
  }{
      $L \gets  1$\;
  }
  \For{$b\gets1$ \KwTo $B$}{
    $\bm{\eta}^{(b)} \gets$ sample $T$ values with replacement from $\{\hat{\bm{\eta}}_t\}_{t = 1}^T$\;
    \For{$t\gets \lfloor \epsilon T \rfloor$ \KwTo $T$}{
    $\hat{m}_t^{(b)} \gets m(\bm{\eta}_t^{(b)})$\;
    $A^{(b)*}_t \gets L\frac{t}{T}\sqrt{T}(\hat{m}_t^{(b)} - \hat{m}_T)$\;
    $K^{(b)}_t \gets (A^{(b)*}_t - \frac{t}{T}A^{(b)*}_T)'(A^{(b)*}_t - \frac{t}{T}A^{(b)*}_T)$\;
    }
    $K^{(b)} \gets \max(K^{(b)}_{\lfloor \epsilon T \rfloor}, \dots, K^{(b)}_{T})$\;
  }
  \KwRet $\{\bm{K}_b\}_{b = 1}^B$;
  \caption{Bootstrap procedure for estimating the distribution of the test statistics under $H_0$}
  \label{alg:breaktest}
\end{algorithm}

The procedure calculates $B$ bootstrap versions of the maximum scaled squared distance between the recursive moments $\hat{m}_t$ and the full sample estimate $\hat{m}_T$. For the parametric copula based test $(G'WG)^{-1}G'W$ is estimated only once using the full sample estimates and the estimation procedures as described in section \ref{s:smmfactorcopulas} and in algorithm \eqref{alg:sigma}. The non-parametric test is based solely on the sample moment functions and does not require the estimation of a copula model. 

# \emph{factorcopula} - an R package for simulation and estimation of factor copulas
\label{s:factorcopulapackage}

In this chapter we focus on the implementation of the previous methods and procedures. With the programming language \emph{R} \parencite{R2017} an open-source package is built, such that the methods can be easily tested, installed and distributed.\footnote{See also the appendix notes on data access in section \ref{s:access}. Explained code examples of the main functions can be found in the appendix section \ref{s:code}.} First, the functions for the configuration and simulation of factor copulas are shortly presented. Second, an overview of the numerical optimization strategy is given. Finally, the validity of the package and the discussed procedures is tested in two simulation studies.

The package consists of a set of high level functions which can be used to construct, simulate and fit various factor copula models. The specification of the factor copula model is handled by the functions `config_factor`, `config_error` and `config_beta`. The two functions `fc_create` and `fc_fit` can be used to either simulate values from a factor copula or to fit a model to a dataset. For conducting the break test as described in section \ref{s:breaktest}, the library offers the functions `fc_critval`, `fc_mstat` and `fc_pstat`. The former simulates critical values for either the moments or copula based test. The latter two calculate the recursive test statistics. A matrix of recursive parameter estimates $\hat{\btheta}_t$ must be provided for the calculation of the copula based test statistic. It can be obtained by recursively applying `fc_fit` to the data. 

Obtaining the estimates is computationally costly. To speed up the estimation, the HPC cluster of the University of Cologne was utilized for all empirical applications\parencite{cheops2018}.

## Copula specification and simulation

With the functions `config_factor`, `config_error` and `config_beta`, the user can define the distribution of the latent variables $Z$ and the error term $\epsilon$ as well as the matrix of factor loadings $\bm{\beta}$. For the specification of the distributions, the function name of any available random number generator (e.g. `rnorm`, `rt`, `rst`) can be used.

Additional arguments such as the distributional parameters must be declared in a named list. The parameters can either be predefined or passed as non-evaluated expressions similar to the internal `formula` interface of the R programming language. To distinguish free model parameters from fixed distributional parameters, an additional character vector with the name of the model parameters has to be passed to `config_factor` and `config_error`. 

The random number generating functions must have an additional argument `n` which defines the number of observations to be simulated. It is not necessary to set this argument explicitly since the number of simulations is controlled by the value $S$ in other functions of the package. 

For the matrix of factor loadings the user can either manually construct a character matrix of parameters or one can use the function `config_beta`. Given a vector `k` and the number of latent variables $K$ this functions constructs a suitable character matrix of zeros and parameter names. The vector $k$ is of length $N$ and defines the group for each observable variable. Therefore, an equidependence model can be specified with $k_N = (1, 1, \dots, 1)$, an unrestricted model with $k_N = (1, \dots, N)$ and a bloc-equidependence model with $k_N = (1, 1, \dots, 2, 2, \dots, M,M, \dots)$, where $M$ is the number of groups. 

Given the copula specifications, the function `fc_create` returns itself a random number generating function. To simulate values from it, the user has to specify a vector `theta` of parameters, the number of simulations $S$ and an optional random seed. Fixing the seed at some integer value can be useful to reproduce the same simulation results over several runs of a program. The vector `theta` must be a \emph{named} vector for which the names correspond to the model parameters specified during the configuration of the model. 

The simulation of new values is a time-consuming part. To avoid unnecessary calls to the underlying random number generators, the function remembers the state of the latent variables $Z$ and the error term $\epsilon$ over repeated function calls and only updates the values if necessary. Thus, if neither the seed nor the distributional parameters in `theta` change, the function uses the same random values from a previous call. This reduces the computation time at the cost of a higher memory consumption. 


## Optimization strategy 
\label{s:optimization}

While the function `fc_create` can be used to simulate values from a factor copula model given a vector of parameters, the function `fc_fit` estimates $\btheta$ via SMM as described in section \ref{s:SMM}. Given a copula specification and the observed residuals, it finds the optimal $\hat{\btheta}$ which minimizes the weighted squared distance between the dependency vectors calculated with simulated data from the factor model and with observable data as presented in equation \eqref{eq:SMM}. 

Optionally, the function also estimates standard errors using the bootstrap algorithm given by \eqref{alg:sigma}. For simplicity, the sample weight matrix $W_T$ is set to the identity matrix. Previous studies showed no significant improvement when the theoretically efficient weight matrix $W_T  = \hat{\bm{\Sigma}}_{T,B}^{-1}$ is used \parencite[694]{Patton2013}. 

As a back end, the method builds on top the `NLopt` optimization library which implements various algorithms for global, local or derivative-free optimization \parencite{nlopt2018, nloptr2014}. The specific choice of the optimization algorithms and the stopping criteria can be altered by the user via the arguments `control.first.stage` and `control.second.stage`. The authors of the library recommend a two-step optimization procedure for global optimization: First, a global optimizer approximates the parameter region in which the global optimum lies. Second, a local derivative free optimizer is then applied using the approximated solution from the first stage. 

By default, `fc_fit` uses the \emph{Multi-Level Single-Linkage} algorithm in the first step. The algorithm creates a sequence of optimal distributed starting values which are then passed to a local optimizer \parencite{Kucherenko2005}. For the local optimizer and the second stage, the \emph{Subplex} algorithm is used, which is based on the popular Nelder-Mead Simplex procedure \parencite{Rowan1990}. 

During the optimization process, many values from the factor copula model are simulated. To avoid numerical instabilities, the random seed is kept fixed, such that always the same random values are drawn \parencite[29]{Gourieroux1996}. As described in the previous section, the memory functionality of `fc_create` can save some computational costs, since redraws from the distributions are avoided if the distributional parameter don't change. This can improve the overall performance of the optimization process. This is especially true, if only the factor loadings $\bm{\beta}$ are optimized while the distributional parameters are kept fixed. In this case, the random number generators are only called once at the beginning of the optimization routine.


## Simulation study
\label{s:simulation}

To illustrate the discussed methods and the validity of the package, two simulation studies are performed: First, an equidependence factor copula model with varying dimensions and sample sizes is estimated repeatedly to show the consistency of the SMM procedure. Second, both the non-parametric and parametric structural break tests are applied once to simulated data from a bloc-equidependence model as described in section \ref{s:restrictions}.

### Convergence for the equidependence model

The DGP of the first study is based on a simple time-invariant equidependence model with standard-normal marginal distributions, one skew-t distributed latent variable and a t-distributed error term. The copula model produces strong asymmetric tail-dependencies. This is achieved by fixing the degrees of freedom at $df = 4$ and the skewness at $\lambda = -0.8$, similar to the upper left graph of figure \ref{fig:equidependence}.  The single factor loading is set to $\beta = 1.5$ and the only free parameter to be estimated is the factor loading. The distributional parameters are kept fixed at their true values. Thus, we can write $\theta = \beta$. The model equations can be written as:

\begin{equation}
\begin{aligned}
Y_i &\sim N(0, 1) ~\forall~ i \in 1, \dots, N\\
Z &\sim \text{skew-t}(df = 4, \lambda = -0.8) \\
\epsilon &\sim \text{t}(df = 4)\\
X_i &= \beta Z + \epsilon\\ 
\end{aligned}
\end{equation}

We repeat the simulation and estimation of $\beta$ over a grid of different values for the number of variables $N$ and the sample size $T$. Each simulation consists of $C = 1000$ Monte-Carlo replications. The number of simulations in the SMM is set to $S = 25000$. Finally, we get a vector of estimates $\hat{\beta}_{t, n, c}$ with $t \in \{100, 1000, 10000\}, n \in \{2, 3, 10\}, c \in \{1, \dots, C\}$. 

\begin{figure}[h]
	\includegraphics[width=\textwidth]{./figures/fig2}
	\caption[Monte-Carlo density estimators for the parameter of an equidependence factor copula model]{Approximated Monte-Carlo density estimators for $\hat{\beta}$ of an equidependence skew t - t factor copula model with $\beta = 1.5$, $S = 25000$, standard normal distributed marginals and different values for $N$ and $T$. Each simulation is based on $1000$ Monte-Carlo replications.}	
	\label{f:montecarlo}  
\end{figure}

Figure \ref{f:montecarlo} shows the results for the first study. For each combination of $N$ and $T$ the kernel density estimator over all Monte-Carlo simulations of is plotted. The bias $\hat{b}_{t, n} = \frac{1}{C}\sum_{c = 1}^{C}  \hat{\beta}_{t, n, c} - \beta$ and the standard deviation is printed in the top left corner. 

The density mass centers around the true value of $\beta = 1.5$. For all simulations the bias is zero or close to it. As the sample size increases, the deviation gets smaller and the SMM estimator converges to the true parameter. But small sample sizes clearly give unreliable results. 

Notably, more variables improve the quality of the estimator as more information is available to estimate the single parameter. This is due to the fact that the number of latent variables and parameters is constant and does not increase with the number of dimensions. As stated in \eqref{eq:equidependence} for the equidependence model one averages over all pairwise dependence vectors. Therefore, the estimation error of the mean decreases if $N$ and the number of pairwise combinations $0.5\dot N\dot(N-1)$ increases.

### Structural break in a bloc-equidependence model
\label{s:break}

For the second study, a more sophisticated model is presented to illustrate the effectiveness of the approach even for high dimensional problems and complicated dependence structures. Analogous to the empirical examples in \textcite{Wied2017} and \textcite{Patton2017} the DGP is based on a \emph{bloc-equidependence} model as described in section \ref{s:restrictions}. 

The model consists of $N = 21$ standard normal distributed variables which are partitioned in $M = 3$ groups of equal size. Each variable is affected by a common and a group specific factor.  The common latent factor is skew-t distributed with strong asymmetric tail-dependency. The three group specific latent factors and the error term are t-distributed. Due to the bloc-equidependence structure, the number of factor loadings reduces from $0.5*N*(N-1) = 210$ to just $2M = 6$. Again, all distributional parameters are kept fixed at their true values and only the factor loadings are estimated such that the recursive estimator can be written as $\btheta_t = (\beta_{1,t}, \dots, \beta_{6,t})'$. 

We chose $T = 1500$, $S = 25 \times T$ and a breakpoint in the parameters of the copula at $t = 1000$. Before the break, $\btheta_{pre} = (0, 1, 1, 0, 1, 1)'$ and after the break $\btheta_{post} = (1.5, 1, 1, 1.5, 1, 1)'$. Thus, only the intra- and interdependence for the first group increases from $0$ to $1.5$ while the remaining factor loadings stay constant.

The test statistics and critical values are based on three different recursive calculations over a range from $t = 300$ to $T = 1500$: First, all parameters are estimated via recursive SMM. Second, only a subset of $\btheta_t$ is estimated recursively while fixing the common and group specific factor for the first group at their full sample estimates $\hat{\beta}_{1, T}$ and $\hat{\beta}_{4, T}$. Hence, for this calculation $\btheta_t = (\beta_{2, t}, \beta_{3, t}, \beta_{5, t}, \beta_{6, t})$. Finally, the non-parametric test statistics based on the moment functions are calculated.

We expect, that a breakpoint is detected around $t = 1000$ only for the first and third recursive calculations. For the second calculation no breakpoint should be detected since the factor loadings of the second and third group are not affected by the simulated structural break.   

Figure \ref{f:breaktest} shows the test statistics of the three different break tests for a single recursive run of the simulation for $t = 300, \dots, 1500$. The horizontal solid line indicates the critical value. Each critical value is calculated using $B = 2000$ bootstrap samples as described in algorithm \eqref{alg:breaktest}. The vertical line indicates the theoretical breakpoint at $t = 1000$.

\begin{figure}[H]
	\includegraphics[width = \textwidth]{./figures/fig3}
	\caption[Parametric and non-parametric structural break test for a bloc-equidependence model]{Parametric and non-parametric structural break test for a bloc-equidependence model with $N = 21$ and $M = 3$ groups of equal size. The theoretical breakpoint is at $t = 1000$ and is modeled as a change of the intra- and interdependence of the first group from $0$ to $1.5$. The first panel shows the copula based test based on all groups. The second panel the copula based test for only the second, and third group which are not affected by the structural break. The lower panel shows the moment based test.}	
	\label{f:breaktest}  
\end{figure}

As expected, both the full copula and the moments based test statistics in the top and lower panel fluctuate strongly. For both tests the maximum distance occurs close at the true breakpoint and lies above the CV. The CV for the moments based test is relatively far away from the maximum while this is not true for the full copula based test. This could indicate that the moments based test is less restrictive. This observation can also be made in the empirical example in the following chapter. The copula based test statistics for the second and third group are clearly below their critical value. Hence, for these groups the null hypothesis of no parameter change cannot be rejected. 

Often, numerical problems arise for large parameter vectors or parameter vectors which include distributional parameters, such as the degrees of freedom or the skewness. Therefore, the test statistics can be substantially distorted with unusual extreme outliers above the CV. We advise to always perform a manual graphical analysis of the final test statistics. In addition, clear outliers could be detected by performing a smoothing method such as running medians or smoothing splines (see also section \ref{s:discussion}). 

Finally, table \ref{tab:simulation} shows the results for a copula model estimated on the full sample, before and after the theoretical breakpoint. For all models we fixed the distributional parameters at their true values. The standard errors $(\frac{1}{T}+\frac{1}{S})\bm{\Omega}$ as given by \eqref{eq:asymptotic} were estimated with $B = 2000$ bootstrap replications and are reported in parentheses. 

\begin{table}[H]
\centering
\begin{tabular}{rlll}
  \toprule
  coefficient & pre-break ($t \leq 1000$) & post-break ($t > 1000)$ & full sample \\
  \midrule
  $\beta_1$ & 0.00 (0.10) & 1.96 (0.69) & 0.00 (0.13) \\ 
  $\beta_2$ & 1.17 (0.37) & 0.88 (0.29) & 1.28 (0.30) \\ 
  $\beta_3$ & 0.96 (0.27) & 0.83 (0.30) & 0.99 (0.27) \\ 
  $\beta_4$ & 0.03 (0.15) & 1.55 (0.64) & 0.88 (0.15) \\ 
  $\beta_5$ & 0.92 (0.18) & 1.01 (0.32) & 0.92 (0.18) \\ 
  $\beta_6$ & 1.04 (0.20) & 1.13 (0.40) & 1.09 (0.20) \\
  \midrule
  $Q$ & 0.0008 & 0.0026 & 0.0025 \\ 
  $T$ & 1000 & 500 & 1500 \\
  $S$ & 37500 & 37500 & 37500 \\
  \bottomrule
\end{tabular}
\caption[Estimation results for the bloc-equidependence simulation]{Estimation results for the bloc-equidependence factor copula model before and after the breakpoint and for the full sample. Standard errors in parentheses (estimated with $ B = 2000$ bootstrap samples).}
\label{tab:simulation}
\end{table}

Comparing the estimates and the value of the objective function at the optimal value, one can notice that the model after the breakpoint and for the full sample are less precise. For the former case, this can be due to the small sample size. For the latter case, this can be due to the misspecified copula, which assumes no change in parameters. The estimates for the pre- and post-break sample are close to their true values. The standard errors for the post-break period are larger due to the smaller sample size. However, the coefficients are still significant. 


# Modelling dependencies in topic salience over time
\label{s:btw17}

In this chapter, we apply the previously discussed methods to a real aggregated dataset collected from the social media platform Facebook. Typically, copula models are applied to model the dependency structures of financial data. For example, the structural break test was used to detect changing dependencies between daily returns of stock market assets during the financial crisis \parencite[18]{Wied2017}. The aim of this analysis is to explore other ways of applying the discussed methods in areas outside from applications in finance and risk management.

The analysis in this paper is broadly related to studies of issue or topic salience in politics and the agenda-setting theory. Issue salience generally refers to the importance of certain topics for individuals or political actors \parencite[557]{Wlezien2005}. Here, by "importance", we refer to the prominence or publicity of an issue. Traditionally, agenda setting is a term which stands for the potency of the media to put policy issues on the public agenda. Therefore, this theory can explain positive dependencies between measures of press media coverage and survey measures of topic salience in a society. The growth of social media channels, blogs and online news sites on the internet has led to a more diverse mechanism of agenda-setting but it also allows for more precise measurements of salience and coverage over time \parencite[193-195]{Neuman2014}. 

Specifically, the case of the German refugee crisis in late 2015 and its perception by the major German parties on the online platform Facebook is analyzed. In 2014 a steady rise of incoming refugees to Germany and Europe could be observed. The majority of them fled from the war in Syria. In 2015 the number of refugees increased rapidly with around 200000 refugees entering Germany in November 2015 (compare to figure \ref{fig:asylum} and \textcite{bpb2018}). 

\begin{figure}[h]
	\includegraphics[width=\textwidth]{./figures/asylum}
	\caption[Monthly asylum seekers and applications from 2014 to 2017]{Monthly asylum seekers and applications from 2014 to 2017. \small{Upon arrival in Germany an asylum seeker is first registered by the authorities (EASY system).  Afterwards, a formal application process can be initialized. The actual number of asylum seekers is smaller, since the old EASY system contains around 13\% - 18\% duplicate entries due to its decentralized structure. After January 2017 in reaction to the crisis, a new registration system was initialized. Source: \textcite{bpb2018}}}	
\label{fig:asylum}  
\end{figure}

Using text data from public social media posts of politicians and parties we derive a measure of issue salience for refugee and asylum related topics. The statistic is calculated separately for each political party and day and hence it measures the importance of the topic within the political discourse of each party. We then analyze whether and how the dependencies between the measures changed due to the refugee crisis. 

In the following, we use the dynamic model as described by \eqref{eq:model} together with the factor copula function as given by \eqref{eq:factorcopula}. It is the same set up as in the previous chapters. First, we give an overview over the raw dataset. Second, we give a description of the feature generation process and present the first stage models of the conditional mean and variance. Third, estimation results for various factor copula models applied to the residual data are presented. Finally, we report the results for both the non-parametric and parametric structural break test. 


## The \emph{btw17} social media dataset

The raw data consists of social media posts published by public pages on Facebook between January 2014 and December 2017. Using the official list of the candidates for the Bundestag election in 2017 (btw17), the account for each of the politicians was manually researched. Only candidates from the six factions in the \emph{Bundestag} (CDU/CSU, SPD, Die Linke, Bündnis 90/ Die Grünen, AfD, FDP) are part of the study. Around 84\% of all 2516 candidates have an account on Facebook \parencite[16]{Stier2018}.

Due to API (application programming interface) and privacy restrictions, only information from public pages could be accessed such that around 52\% of the social media accounts could be considered for the data collection. In addition to the candidates pages, $113$ official pages from the political parties, both on the federal and regional level, were included. 

The data collection took place on several days between 2017-11-21 and 2018-02-06. A web-scraping software was built on top of the \emph{restfb} Java client library. It made calls to Facebook's official Graph API \parencite{restfb2018}. The posts are stored in a document orientated database on cloud-servers located in Germany. Besides the actual content of the post, it includes a time-stamp, the user-id of the author and the number of likes, shares and comments it has received upon collecting it from the API.

For this analysis, the data is restricted on cleaned textual posts only.\footnote{A post can also contain a photo, an album or an event with no additional text. The post's text was cleaned by removing links and stop-words, transforming umlauts and converting it to lowercase letters.}. This results in almost 664 thousand posts tagged with the party membership of their authors. The first two panels of figure \ref{f:nPosts} show the monthly number of active accounts and the monthly number of posts for each party. An account was defined active if it published at least one post for a month considered. 

In early 2014, approximately 500 accounts were active. This number increased steadily to roughly 750 accounts in mid-2016. From then until the election in September 2017 the number increased rapidly to almost 1200 active accounts followed by a drop after the election. This trend has two reasons: First, we only collected accounts from politicians which were candidates in the btw17. Therefore, other politicians which were only active before the btw17 are not considered here. Second, we suspect that many politicians opened an account just for the election campaign. After the election many candidates closed their accounts due to a failure in the election or because campaigning time was over. 

A similar pattern can also be observed for the monthly number of posts. Until the election year, roughly 10000 posts from political parties were published per month. In the month of the election it was around 4 times more. 

The bottom panel of figure \ref{f:nPosts} shows the absolute number of monthly posts related to refugee and asylum issues. As in \textcite[192-198]{Neuman2014} we define a key identifying term for an issue and simply use it to search and filter the data. 

Specifically, we define a post to be "refugee related" if it matches the regular expression \texttt{flucht|fluecht}. For example, a match occurs if a post contains the words \emph{flüchtlingskrise} (refugee crisis), \emph{fluchtursachen} (causes of flight), \emph{flüchten} (to flee) or \emph{flüchtlingsheime} (refugee hostels). 
A sharp rise of refugee related posts can be observed in autumn and winter of 2015 and 2016. Notably, this pattern is very similar to the overall number of refugees who entered Germany in 2015 with the peak occurring during in an interval of only a few weeks (compare to figure \ref{fig:asylum}). 

\begin{figure}[h]
	\includegraphics[width=\textwidth]{./figures/nPosts}
	\caption[Number of active accounts and posts per party and month]{Number of active accounts and posts per party and month. The bottom panel shows the number of posts matching the regular expression \texttt{flucht|fluecht}. The vertical solid and dashed lines indicate the breakpoint detected by the non-parametric moments and the parametric copula based test.}	
	\label{f:nPosts}  
\end{figure}

Table \ref{t:facebook} summarizes some overall aggregated statistics for each party. It shows the overall number of posts, accounts, summed up likes and shares for posts and the within party share of refugee related posts over the observation period from 2014-01-01 to 2017-12-31. Although the right and left wing parties \emph{AfD} and \emph{Linke} have a relatively small number of accounts and posts they generate by far the greatest number of attention and engagement in terms of likes and shares. The social democratic party \emph{SPD} has over twice more posts than the \emph{AfD} but roughly generates only half of the likes and only a fifth of the shares. Looking at the share of "refugee related" posts one can see that both the far left and far right parties talk more about this topic than the average.

\begin{table}[h]
\centering
\begin{tabular}{rccccc}
  \toprule
 party & posts & accounts & likes & shares & share of posts related to  \\ 
 & & &\multicolumn{2}{c} {(in Million)} &  "refugees" (in \%) \\ 
\cmidrule(r){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(l){6-6}
AfD & 74724 & 162 & 18.36 & 7.59 & 6.96 \\ 
  CDU/CSU & 169115 & 267 & 14.72 & 1.83 & 3.49 \\ 
  FDP & 71083 & 201 & 6.32 & 0.77 & 2.39 \\ 
  Grüne & 67188 & 139 & 4.03 & 1.28 & 4.62 \\ 
  Linke & 84723 & 158 & 16.03 & 4.23 & 6.23 \\ 
  SPD & 196805 & 290 & 10.11 & 1.57 & 3.66 \\
\cmidrule(r){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(l){6-6}
  All parties & \numprint{663638} & 1217 & 69.57 & 17.27 & 4.28\\
  \bottomrule
\end{tabular}
\caption[Overall number of posts, active accounts, likes and shares for the btw17 dataset]{Overall number of posts, active accounts, likes and shares over the observation period from "2014-01-01" - "2017-12-31". The last two columns show the fraction of posts matching the regular expression \texttt{flucht|fluecht}.}
\label{t:facebook}
\end{table}

## Data processing and descriptive analysis

Separately for each party we count the daily number of "refugee related" posts and divide it by the overall daily number of posts from this party. Thus, for each day and party, we get a relative within-party frequency of refugee related posts. We use this statistic to measure the issue salience for each party. For example, a value close to 1 indicates that all discussions are related to refugee and asylum issues while a value close to 0 indicates that this topic is of no importance in the political discourse. 

First, the univariate models for each of the six salience measures are estimated. The factor copula dependency analysis is then performed on the residual information. The EDF is used to model the marginal distributions of the residuals. 

Standard ARIMA-GARCH models are applied to the univariate time series. In this type of models the conditional mean $\mu_t = E(Y_t|\mathcal{F}_{t-1})$ of some possibly integrated variable is assumed to be a linear function of its own lags and lags of the error term. To also account for volatility clustering, the conditional variance $\sigma_t^2 = V(Y_t|\mathcal{F}_{t-1})$ is assumed to be a linear function of its own lags and squared lags of the error term \parencite[17-19]{Teraesvirta2009}. 

For the conditional variance we assume a GARCH(1, 1) process. For the model of the conditional mean we estimate different ARIMA models. The orders of the auto-regressive and moving average terms of the mean model are determined by testing various candidates over a small grid of order values. The final model is selected by comparing the Bayesian information criteria (BIC). Table \ref{tab:ARIMA} summarizes the final parameter values for the six time series model applied to the measures of topic salience.

\begin{table}[H]
\centering
\begin{tabular}{rcccccc}
  \toprule
  Party & AfD & CDU/CSU & FDP & Grüne & Linke & SPD \\
  \midrule
  AR & 4 & 1 & 2 & 3 & 2 & 1\\
  I & 1 & 1 & 1 & 1 & 1 & 1\\
  MA & 1 & 2 & 1 & 3 & 2 & 1\\
  \bottomrule
\end{tabular}
\caption[Parameters for the ARIMA-GARCH(1,1) model]{Parameters for the ARIMA-GARCH(1,1) model used to model the marginal distributions of the btw17 dataset.}
\label{tab:ARIMA}
\end{table}

\begin{figure}[H]
	\subfloat[Relative daily within-party frequency of refugee related posts.]{\includegraphics[width = \textwidth]{./figures/flucht_observed}} \\
	\subfloat[Residuals of ARIMA-GARCH models applied to the time series.]{\includegraphics[width= \textwidth]{./figures/flucht_residuals}}
	\caption[Univariate time series and estimated residuals for the btw17 dataset]{Univariate time series and estimated residuals for the btw17 dataset. The upper panel shows the daily within-party frequency of refugee related topics. The residuals are estimated by applying ARIMA-GARCH models to the time series.}	
	\label{f:residuals}  
\end{figure}

Figure \ref{f:residuals} shows the time series of topic salience and the residuals for each of the six parties after applying the ARIMA-GARCH(1, 1) models. In line with the bottom panel of figure \ref{f:nPosts}, refugee related topics start to become important in mid 2015 with peaks of up to 40\% importance in late 2015. Especially for the left and right wing parties, "Linke" and "AfD", the topic stays important even after the crisis. 

Finally, in table \ref{t:correlations} several pairwise sample dependencies for the residuals are presented. The same dependency statistics as for the SMM procedures as described in section \ref{s:smmfactorcopulas} are used. Looking at the rank correlation one finds only very weak to weak positive dependencies of up to $0.20$ for the two largest parties CDU/CSU and SPD. The quantile dependencies are all larger 0 but relatively small. Distinct patterns of asymmetric tail dependencies are not visible. 

\begin{table}[H]
\centering
\begin{tabular}{rccccc}
  \toprule
  & Rank - & \multicolumn{4}{c}{Quantile-dependence}\\
 Pairs &  correlation & 0.05 & 0.1 & 0.90 & 0.95 \\ 
  \cmidrule(r){1-1} \cmidrule(rl){2-2} \cmidrule(l){3-6}
AfD-CDU/CSU & 0.07 & 0.14 & 0.17 & 0.13 & 0.10 \\ 
  AfD-FDP & 0.08 & 0.10 & 0.14 & 0.13 & 0.07 \\ 
  AfD-Grüne & 0.03 & 0.07 & 0.16 & 0.09 & 0.10 \\ 
  AfD-Linke & 0.08 & 0.07 & 0.15 & 0.19 & 0.08 \\ 
  AfD-SPD & 0.09 & 0.11 & 0.16 & 0.16 & 0.08 \\ 
  CDU/CSU-FDP & 0.17 & 0.16 & 0.25 & 0.18 & 0.14 \\ 
  CDU/CSU-Grüne & 0.13 & 0.11 & 0.20 & 0.14 & 0.07 \\ 
  CDU/CSU-Linke & 0.10 & 0.10 & 0.18 & 0.16 & 0.14 \\ 
  CDU/CSU-SPD & 0.20 & 0.22 & 0.22 & 0.19 & 0.14 \\ 
  FDP-Grüne & 0.10 & 0.08 & 0.19 & 0.15 & 0.10 \\ 
  FDP-Linke & 0.10 & 0.10 & 0.16 & 0.17 & 0.10 \\ 
  FDP-SPD & 0.12 & 0.19 & 0.23 & 0.18 & 0.07 \\ 
  Grüne-Linke & 0.09 & 0.07 & 0.12 & 0.19 & 0.08 \\ 
  Grüne-SPD & 0.12 & 0.14 & 0.21 & 0.16 & 0.14 \\ 
  Linke-SPD & 0.15 & 0.16 & 0.16 & 0.23 & 0.21 \\ 
  \cmidrule(r){1-1} \cmidrule(rl){2-2} \cmidrule(l){3-6}
  Average & 0.11 & 0.12 & 0.18 & 0.16 & 0.11 \\ 
   \bottomrule
\end{tabular}
\caption[Pairwise sample dependencies for the daily topic salience of each party]{Pairwise sample dependencies for the daily topic salience of each party applied to the standardized residuals.}
\label{t:correlations}
\end{table}

## Results

To get a first idea of the characteristics of the data various factor copula models are fitted to the complete sample. To determine the number of latent factors $K$, we use an approach by \textcite[p. 148]{Patton2017} and analyze the ordered eigenvalues of the residual rank-correlation matrix. One eigenvalue is above the threshold. Therefore, the analysis is restricted to one-factor copula models (see also figure \ref{f:eigenvalues}). We separately fitted equidependence and unrestricted models for various distributions of the factor and error term. Table \ref{tab:onefactor} summarizes the estimation results for the factor copula models.

\begin{table}[H]
\centering
\begin{tabular}{rllllll}
  \toprule
  & \multicolumn{3}{c}{Equidependence} & \multicolumn{3}{c}{Unrestrictive} \\ 
  & norm-norm & t-t & skewt-t & norm-norm & t-t & skewt-t\\
  \cmidrule(r){2-4} \cmidrule(l){5-7}
  $\beta_1$ & 0.43 & 0.42 & 0.42 & 0.23 & 0.23 & 0.25 \\ 
  $\beta_2$ &      - &      - &      - & 0.62 & 0.52 & 0.63 \\ 
  $\beta_3$ &      - &      - &      - & 0.42 & 0.44 & 0.43 \\ 
  $\beta_4$ &      - &      - &      - & 0.32 & 0.34 & 0.33 \\ 
  $\beta_5$ &      - &      - &      - & 0.37 & 0.41 & 0.35 \\ 
  $\beta_6$ &      - &      - &      - & 0.63 & 0.60 & 0.67 \\ 
  \cmidrule(r){2-4} \cmidrule(l){5-7}
   $df$ & - & 99 & 96 &  - & 58 & 38 \\ 
  $\lambda$ & - & - & -0.29 & - & - & -0.36 \\ 
  \cmidrule(r){2-4} \cmidrule(l){5-7}
  $Q$ & 0.0036 & 0.0034 & 0.0031 & 0.1081 & 0.0979 &  0.0903\\ 
  \bottomrule
\end{tabular}
\caption[Estimation results for different one-factor copula specifications]{Estimation results for different one-factor copula specifications applied to the residuals of the btw17 dataset.}
\label{tab:onefactor}
\end{table}

For the equidependence models, the single factor loading is similar but relatively small for all three models. The degrees of freedom for both the t and skew-t model are large. This indicates, that no tail dependencies are present. The best fit in terms of the $Q$ value of the objective function is given by the skew-t model. The skewness parameter is negative which could indicate stronger dependencies in the lower parts of the distribution. 

The nonrestrictive models all have a larger $Q$ value. However, the factor loadings are relatively stable between the models, ranging from 0.23 for the AfD party to around 0.65 for the two largest parties SPD and CDU/CSU. Calculating the average of the factor loadings for each unrestricted model yields a similar value compared to the estimates of the equidependence model. The degrees of freedom are still large but smaller compared to the equidependence model. The skew-t model is again characterized by negative asymmetries. 

For the recursive estimation of the parameters, we proceed with the equidependence model which has the lowest $Q$ value of all estimated models. To avoid numerical instabilities, we fix the distributional parameters $df$ and $\lambda$ at their full sample estimates. Therefore, we only test for a break in the single factor loading. Possible breaks in the tail dependency or symmetry are left out. 

Four break tests are performed: A restrictive and non-restrictive non-parametric test based on the moment functions and two parametric tests based on the recursive estimates of a skewt-t and a norm-norm factor copula's loadings. Table \ref{tab:breakpoint} summarizes the results. 

\begin{table}[H]
\centering
\begin{tabular}{rcccc}
  \toprule
  & \multicolumn{2}{c}{Moments based test} & \multicolumn{2}{c}{Equidep. copula based test} \\ 
  & unrestricted & restricted & skewt-t & norm-norm \\
  \cmidrule(r){1-1} \cmidrule(lr){2-3} \cmidrule(l){4-5}
  test statistic & 6.05 &            116.95  &   1.51 & 1.85 \\ 
  95\%-CV  &      1.98 (0.00) &       61.90 (0.00) & 1.67 (0.069) & 1.60 (0.028) \\ 
  breakpoint      &  2015-09-18    &  2015-09-18  & 2015-03-13 & 2015-03-11\\ 
  \bottomrule
\end{tabular}
\caption[Results for the parametric and non-parametric break point detection tests]{Results for the parametric and non-parametric break point detection tests applied to the residuals of the btw17 dataset (p-value in parentheses).}
\label{tab:breakpoint}
\end{table}

The two non-parametric test clearly detect a breakpoint in the dependence structure for 2015-09-18.\footnote{The dates of the breakpoints are also marked as vertical lines in figure \ref{f:nPosts}.} Around this date, the fraction of posts which include refugee related topics was the highest for the sample. The test statistics are around two to three times larger than the critical value. For the critical value the type I error was set to $\alpha = 0.05$. The test based on the skewt-t copula parameter cannot reject the null hypothesis. The p-value of around $0.07$ is sightly larger than the chosen alpha value of $0.05$. The highest value of the test statistic occurred at 2015-03-13 a few months before the date detected by the non-parametric tests. Conducting the break test based on a one factor equidependence norm-norm factor copula yields a p-value of $0.028$ for a breakpoint around 2015-03-11. 

As already seen in the simulation study in section \ref{s:break}, the non-parametric test seems to be less sensitive. The different results can also be explained by the fact, that the copula based tests only tested for a break in the single factor loading. To better assess the results, we further estimated the dependence structure before and after the break using a skew-t equidependence factor copula (see table \ref{tab:models}). We choose the breakpoint detected by the moment based tests since this has the lowest p-value. For convenience, the full sample estimates as stated in table \ref{tab:onefactor} are replicated here. For all parameters, standard errors were estimated with the bootstrap procedure. 

We excluded the estimation of the degrees of freedom and used the full sample estimates. Estimating this distributional parameter often resulted in very large standard errors and unstable numerical results. In this setting, the parameter estimates also changed if the \emph{inverse} degrees of freedom over the range of $[0, 0.5)$ instead of the regular degrees of freedom over the range $(2, \infty]$ were estimated. 

\begin{table}[H]
\centering
\begin{tabular}{rlll}
  \toprule
  coefficient & before ($t \leq 626$) & after ($t > 626)$ & full model \\
  \midrule
  $\beta_1$ & 0.39 (0.03) & 0.41 (0.03) & 0.42 (0.03) \\ 
  $\lambda$ & -0.55 (0.75) & -0.33 (0.54) & -0.29 (0.37) \\
  \midrule
  $Q$ &  0.0029&  0.0023 & 0.0031 \\ 
  $T$ & 626 &  835 & 1461 \\
  $S$ & 36525 & 36525 & 36525 \\
  \bottomrule
\end{tabular}
\caption[Post and pre-break estimation results for the btw17 dataset]{Estimation results for the btw17 dataset. A bloc-equidependence factor copula model is estimated before and after the breakpoint at $t = 626$ (2015-09-18). Standard errors in parentheses (estimated with $ B = 2000$ bootstrap samples). Degrees of freedom were fixed at the full model estimate of $df = 96$. }
\label{tab:models}
\end{table}

For all three models, the skewness parameters $\lambda$ are negative. However, comparing the size of the estimates to their standard errors shows that the skewness parameters are not significantly different from 0. The estimates for the factor loading is significant and does not fluctuate between the pre and post-break period. Despite the "breakpoint" detected by the moment based tests, the pre and post-break copula models seem to be very similar.  

Given the large degrees of freedom and the insignificant skewness parameter, it seems that the dependence structure can be modeled as a one factor equidependence copula for which the latent variable is normal distributed. This special case of a one-factor copula model can also be described with a Gaussian copula. Estimating the pre- and post-break Gaussian one-factor model for the earlier breakpoint detected by the parametric break test at 2015-03-11 yields an increase in the factor loading from $\hat{\beta}_{pre} = 0.35$ to $\hat{\beta}_{post} = 0.54$. This indicates an increase in positive dependencies for the time of the refugee crisis and afterwards. 

# Discussion and summary
\label{s:discussion}

In this chapter, the presented methods are shortly reviewed, the results are critically assessed and finally we give some outlook on future improvements and research. 

The results of the empirical chapters \ref{s:simulation} and \ref{s:btw17} are all based on a general statistical model for analyzing the dependence structure of multivariate time series. This model allows for a multistage estimation process: First the time varying conditional means and variances of the marginal distributions are estimated using some parametric model for univariate time series. Second, the standardized residuals are obtained and their multivariate cdf is modeled using the non-parametric EDF for the marginal distributions and a parametric copula model for the dependence structure.

In this paper, the focus was put on the special class of factor copula functions. This type of copula function is based on a linear factor structure for which the dependencies are described by a lower dimensional set of latent variable. The parameters of a factor copula model involve the matrix of factor loadings and the distributional parameters of the latent factors and the error term. The choice of the distributions and the number of latent factors are hyper-parameters of the model which have to be determined prior to the estimation.

Since the parameter vector of a factor copula model can grow very large, two model restrictions were discussed: The equidependence model which assumes equal pairwise dependencies for all variables and the bloc-equidependence model. For the latter, it is assumed that the variables can be naturally partitioned into groups and that the pairwise intra-group and inter-group dependencies are equal. 

The latent factor model results in complex dependence structures and in most cases a closed form mapping from the model parameters to measures of dependency doesn't exist. In addition, obtaining the likelihood function of a factor copula model is difficult and computationally expensive. However, it is computationally cheap to simulate random values from the factor model. This leads to the Simulated Method of Moments estimation procedure: This method is rooted in GMM estimation but the population moment function is replaced by a sample equivalent calculated with simulations from the model. The classical GMM and SMM are based on functions involving moments of the variables. But the SMM for factor copulas is based on scale-free measures of dependencies as functions of the variable's EDFs. Nevertheless, asymptotic results can still be derived and it can be shown that the parameter estimates are asymptotically normal distributed. 

For many applications it is of interest whether the dependence structure is time invariant. Therefore, a structural break test for a change in the parameters of the factor copula model was presented. It requires the recursive estimation of the factor copula parameters and compares them to the full sample estimates. If the distance is significantly large, a structural break is detected. The parametric test allows for the detection of breaks in subsets of the parameter vector but it is computationally costly due to its recursive model estimates. As an alternative, the test can also be formulated non-parametrically involving only finite sample versions of dependency measures applied to the residual data. 

To enable other researcher to assess and enhance the discussed methods a software library for the programming language \emph{R} is provided. To confirm the validity of the methods and their implementations, two simulation studies were performed. The first study demonstrated the convergence properties of the copula parameters for an equidependence model. Increasing the sample size or the number of observations reduced the variance of the estimator. The second study simulated a structural break in the copula for a bloc-equidependence model with 21 variables in 3 different groups. Both the non-parametric and the parametric test are able to detect the breakpoint with high accuracy. 

The simulation studies in this paper show that the factor copula model is theoretically suitable for modelling high dimensional dependence structures. However, in contrast to classical GMM estimation, the objective function of the copula's SMM estimator is not continuous and differentiable. Therefore, one can only rely on derivative free optimization algorithms. This reduces the performance and stability of the estimation process. The computational demand is especially high for the parametric break test which requires the recursive estimation of a copula model for almost all points in time. 

To test the methods on real world data, a case study with textual data from the social media site Facebook is performed. The data consists of public posts of German politicians and political parties from 2014 to 2017. For each major German political party we derive dynamic measures of topic salience for refugee and asylum related issues and ask whether the refugee crisis in late 2015 influenced the dependence structure between the measures of topic salience.

Both the non-parametric and the parametric break test based on a norm-norm one factor equidependence copula detected a breakpoint in the parameters of the factor copula for March and September 2015 respectively. November 2015 marks the peak of the refugee crisis with around 200000 refugees entering Germany. For each break date a factor copula is estimated before and after the break. For the later breakpoint, detected by the non-parametric test, no change in the copula parameters can be found. This finding somehow contradicts the result of the test. However, for the early parametric breakpoint, an increase in the single factor loading by around 48% can be observed. 

This finding indicates that in 2014 and early 2015, just before the peak of the refugee crisis, the dependencies were weaker compared to the time of the crisis and afterwards. Before the crisis the within-party importance of refugee and asylum related issues was more independent of the political discourse of the other parties. However, the external shock of the crisis led to a stronger convergence of the issue salience.

The real-data application has several drawbacks: Interpreting the actual size of the factor loadings is difficult. That hinders the assessment of actual real changes due to the breakpoint and allows only for relatively fuzzy conclusions. In addition, we forwent the implementation and conduction of a so called J test for over-identifying restrictions for the equidependence model \parencite[693]{Patton2013}. 

The undesirable properties of the objective function require the usage of global and local derivative free optimization algorithms. During the empirical analysis we regularly encountered unstable numerical results. For the break test, this can affect the recursive test statistics and extreme outliers above the critical value are possible. Numerical instabilities arise especially if some distributional parameters such as the skewness or the degrees of freedom are included in the estimation process. It seems that it requires much more data than typically available to estimate these parameters consistently. 

Therefore, much care has to be taken in the optimization process and the tuning parameters of the optimization algorithms. Future research should concentrate on improving the stability of the SMM method and the performance of estimating many recursive models for the structural break test. For example, a promising approach is undertaken by \textcite{Frazier2017}. Their method tries to overcome the problem of discontinuity by proposing an adapted simulation algorithm which allows the application of automatic differentiation techniques and hence allows the use of efficient derivative-based optimization procedures.

\newpage
\appendix

# Appendix


## Notes on data and source code access
\label{s:access}

An online version of this thesis is publicly available as a git-repository under https://github.com/bonartm/factorcopula-thesis. The repository contains notes on how to install all dependencies. The source code files for the simulation studies and analyses are located online in the `source` folder. 

Due to data restrictions by Facebook it is not possible to publish the original dataset of Facebook posts. However, the residuals of the ARIMA-GARCH models which were applied to the aggregated Facebook data are located online at `data/topics_residuals.rds`. 

Almost all estimation procedures (for instance the bootstrap algorithms and the recursive model estimation) are embarrassingly parallel. Therefore, the HPC cluster of the University of Cologne was utilized to massively speed up the estimation process \parencite{cheops2018}. 

To simplify the workflow and the communication with the cluster, we wrote the R-package `cheopsr`. It allows the execution of parallel jobs and the collection of results from within the local R environment. The package is available online at https://github.com/bonartm/cheopsr. To run the package, a Unix-like system and access rights to the HPC cluster are obligatory. 

The methods for the simulation and estimation of factor copula models and the break test are available via a separate R-package `factorcopula`. The package can be easily downloaded and installed from Github. Further notes can be found under: https://github.com/bonartm/factorcopula. 

## Code examples
\label{s:code}

In the following explained example, the \emph{R} package \emph{factorcopula} is used to define a bloc-equidependence factor copula with three latent variables. Data from the model is simulated for some specific vector $\btheta$. Finally, the parameter vector is estimated with the SMM.  

The first factor is skewed-t distributed, the second and third are standard normal distributed. The error term is t distributed with 4 degrees of freedom. The distributional parameters `dfInv` and `lambda` of the skewed-t distribution are free parameters of the model. 
```{r}
library(factorcopula)
Z <- config_factor(rst = list(nu = 1/dfInv, lambda = lambda), 
                   rnorm = list(),
                   rnorm = list(),
                   par = c("dfInv", "lambda"))
eps <- config_error(rt = list(df = 4))
```

A model with $K=3$, $N = 6$, $k_1 = k_2 = 3$ and $M = 2$ is constructed. Together with the specification of the latent variables this results in a bloc-equidependence model with a skewed-t distribution for the common factor and a standard normal distribution for each of the two group specific factors. 
```{r}
k <- c(1, 1, 1, 2, 2, 2)
beta <- config_beta(k, 3)
beta
```

Next, $S = 1000$ random values from the copula model are simulated and the marginal distributions are transformed to standard normal ones. The factor loadings for the common factor are $\beta_1 = \beta_2 = 1$ and the group specific loadings $\beta_3 = \beta_4 = 1.5$. The distributional parameters of the common latent factor are set to $dfInv = 0.25$ and $\lambda = -0.8$. This produces asymmetric tail dependencies.
```{r, eval = FALSE}
copfun <- fc_create(Z, eps, beta)
theta <- c(beta1 = 1, beta2 = 1, beta3 = 1.5, beta4 = 1.5, 
           dfInv = 0.25, lambda = -0.5)
Y <- qnorm(copfun(theta, S = 1000))
```

Finally, we define some lower and upper bounds for the numerical estimates and approximate the parameters of the model via SMM. Standard errors are estimated with 1000 bootstrap samples. To get more precise parameter estimates, the stopping criteria for the first or second stage optimization algorithms can be increased. 
```{r, eval = FALSE}
lower <- c(beta1 = 0, beta2 = 0, beta3 = 0, beta4 = 0, 
           dfInv = 0.01, lambda = -0.9)
upper <- c(beta1 = 5, beta2 = 5, beta3 = 5, beta4 = 5, 
           dfInv = 0.49, lambda = 0.9)
model <- fc_fit(Y, Z, eps, beta, lower, upper, k = k,
                S = 25000, se = TRUE, B = 1000)
round(model$theta.second.stage, 2)
# beta1  beta2  beta3  beta4  dfInv lambda 
#  1.26   0.96   1.40   1.64   0.30  -0.55 
round(model$se, 2)
# beta1  beta2  beta3  beta4  dfInv lambda 
#  0.52  0.44   0.44   0.44   0.23   0.86 
```

## Additional figures

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth]{./figures/pairwise_scatter}
	\caption{Pairwise scatter-plot of the estimated residuals for the btw17 dataset}	
	\label{f:pairwise_scatter}  
\end{figure}

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth]{./figures/eigenvalues}
	\caption{Scree-plot of ranked eigenvalues based on the pairwise rank-correlation matrix}	
	\label{f:eigenvalues}  
\end{figure}










